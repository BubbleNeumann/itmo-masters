{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Model Lab: Building Stable Diffusion from Scratch\n",
        "\n",
        "(adapted from Harvard seminars)\n",
        "\n",
        "## Introduction\n",
        "Welcome to the Diffusion Models Laboratory! In this lab, you will implement essential components\n",
        "of a Stable Diffusion-like model and have a working text-to-image generation system by the end.\n",
        "\n",
        "## Learning Objectives\n",
        "By completing this lab, you will:\n",
        "1. Understand the theory behind diffusion models\n",
        "2. Implement key components of a diffusion-based generative model\n",
        "3. Learn how to train and sample from diffusion models\n",
        "4. Experience how attention mechanisms enable text-conditioning\n",
        "5. Explore the efficiency benefits of latent diffusion models\n",
        "\n",
        "## Lab Overview\n",
        "This lab will guide you through building a complete diffusion-based generative model with the following components:\n",
        "1) Forward and reverse diffusion processes for generating new content\n",
        "2) Neural networks with appropriate inductive biases for images (U-Net architecture)\n",
        "3) Cross-attention mechanisms to bridge text and visual features\n",
        "4) Latent spaces for efficient image representation via autoencoders\n",
        "\n",
        "We'll use the MNIST dataset (28x28 pixel handwritten digits) as our training data to make the\n",
        "process reasonably fast. By the end, your model will convert a text prompt like \"7\" into\n",
        "an image of the handwritten digit 7.\n",
        "\n",
        "## Lab Structure\n",
        "This lab is divided into several tasks, each focusing on a different component of diffusion models:\n",
        "- Task 1: Implement forward and reverse diffusion processes\n",
        "- Task 2: Implement the U-Net architecture for diffusion\n",
        "- Task 3: Develop loss functions and sampling mechanisms\n",
        "- Task 4: Build attention mechanisms for conditioning\n",
        "- Task 5: Create a latent diffusion model with an autoencoder\n",
        "\n",
        "For each task, you'll find:\n",
        "- Theoretical background explaining key concepts\n",
        "- Starter code with TODOs for you to complete\n",
        "- Testing functions to verify your implementations\n",
        "- Visualization helpers to understand what's happening\n",
        "\n",
        "## Managing computational resources\n",
        "If you encounter resource limitations (e.g., end of Colab quota or VPN issues):\n",
        "- Use CPU for training (can be done on Colab or your laptop)\n",
        "- Implement checkpointing to save model progress (save checkpoints to Google Drive in Colab)\n",
        "- Load from checkpoints to continue training from where you left off\n",
        "\n",
        "### Google Drive Integration\n",
        "When running this lab in Google Colab, you can save your model checkpoints to Google Drive, which prevents\n",
        "losing your training progress if your Colab session crashes or times out. To enable this:\n",
        "\n",
        "1. Set `USE_GOOGLE_DRIVE = True` near the top of the code\n",
        "2. When prompted, authorize the connection to your Google Drive\n",
        "3. Your checkpoints will be saved to the \"diffusion_model_checkpoints\" folder in your Drive\n",
        "\n",
        "You can always resume training from these checkpoints even if you start a new Colab session.\n",
        "\n",
        "Let's get started with building diffusion models!"
      ],
      "metadata": {
        "id": "SAkqLz21x6kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required library imports\n",
        "import functools\n",
        "import os  # Added for file operations\n",
        "import time  # Added for timing operations\n",
        "import math\n",
        "from pathlib import Path  # Added for path handling\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "viwKukETyFF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Google Colab\")\n",
        "\n",
        "# Function to setup Google Drive for checkpoint storage\n",
        "def setup_checkpoint_dir(use_google_drive=False):\n",
        "    \"\"\"\n",
        "    Sets up the directory for storing model checkpoints.\n",
        "\n",
        "    Args:\n",
        "        use_google_drive: If True and running in Colab, mounts Google Drive\n",
        "                         and creates checkpoint directory there\n",
        "\n",
        "    Returns:\n",
        "        Path to the checkpoint directory\n",
        "    \"\"\"\n",
        "    if use_google_drive and IN_COLAB:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            # Mount Google Drive\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "            # Create checkpoint directory in Google Drive\n",
        "            checkpoint_dir = \"/content/drive/MyDrive/diffusion_model_checkpoints\"\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            print(f\"Checkpoint directory set to Google Drive: {checkpoint_dir}\")\n",
        "            return checkpoint_dir\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to set up Google Drive for checkpoints: {e}\")\n",
        "            print(\"Falling back to local checkpoint directory\")\n",
        "\n",
        "    # Use local directory if not using Google Drive or if setup failed\n",
        "    checkpoint_dir = \"model_checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(f\"Checkpoint directory set to local path: {checkpoint_dir}\")\n",
        "    return checkpoint_dir"
      ],
      "metadata": {
        "id": "SBNzlojnyHjU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "uaa5VFnuyORs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Use Google Drive for checkpoints when running in Colab\n",
        "# Set to True to save checkpoints to Google Drive\n",
        "USE_GOOGLE_DRIVE = False  # Change to True to use Google Drive for checkpoints\n",
        "\n",
        "# Create directory for saving models\n",
        "CHECKPOINT_DIR = setup_checkpoint_dir(use_google_drive=USE_GOOGLE_DRIVE)"
      ],
      "metadata": {
        "id": "A1exTfMsySWu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Forward and Reverse Diffusion Processes\n",
        "\n",
        "The core concept of diffusion models is to gradually add noise to data samples until they become\n",
        "pure noise, then learn to reverse this process. This allows us to generate new samples by starting\n",
        "with random noise and applying the learned denoising process.\n",
        "\n",
        "### Forward Diffusion\n",
        "In forward diffusion, we gradually add noise to our data samples according to:\n",
        "\n",
        "x(t + Δt) = x(t) + σ(t) √(Δt) * ε\n",
        "\n",
        "where:\n",
        "- σ(t) is the noise strength at time t\n",
        "- Δt is the step size\n",
        "- ε ~ N(0, 1) is a standard normal random variable\n",
        "\n",
        "As t increases, our sample becomes increasingly noisy until it's indistinguishable from pure noise.\n",
        "\n",
        "### Task 1.1: Implement the Forward Diffusion Process"
      ],
      "metadata": {
        "id": "SwPOoLOEyWSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example noise strength function: constant value regardless of time\n",
        "def constant_noise_strength(t):\n",
        "    \"\"\"Returns a constant noise strength regardless of time.\"\"\"\n",
        "    return 1.0\n",
        "\n",
        "# TODO: Implement the forward diffusion process\n",
        "def forward_diffusion_1D(initial_sample, noise_strength_fn, initial_time, num_steps, step_size):\n",
        "    \"\"\"\n",
        "    Simulates the forward diffusion process in 1D.\n",
        "\n",
        "    Args:\n",
        "        initial_sample: Starting value (scalar)\n",
        "        noise_strength_fn: Function that returns noise strength at given time\n",
        "        initial_time: Starting time value\n",
        "        num_steps: Number of diffusion steps to simulate\n",
        "        step_size: Size of each time step\n",
        "\n",
        "    Returns:\n",
        "        tuple: (trajectory array, time array)\n",
        "    \"\"\"\n",
        "    # Initialize arrays to store the trajectory and time values\n",
        "    trajectory = np.zeros(num_steps + 1); trajectory[0] = initial_sample\n",
        "    time_values = initial_time + np.arange(num_steps + 1)*step_size\n",
        "\n",
        "    # Perform Euler-Maruyama steps for the diffusion process\n",
        "    for i in range(num_steps):\n",
        "        #  1. Get the current noise strength using noise_strength_fn\n",
        "        noise_strength = noise_strength_fn(time_values[i])\n",
        "\n",
        "        ############# YOUR CODE HERE (2 lines)\n",
        "        #  TODO: 2. Sample random noise from a standard normal distribution\n",
        "        #  TODO: 3. Update the trajectory according to the forward diffusion equation\n",
        "        #############\n",
        "\n",
        "    return trajectory, time_values\n",
        "\n",
        "\n",
        "# Test the forward diffusion function\n",
        "def visualize_forward_diffusion():\n",
        "    \"\"\"Visualize multiple forward diffusion trajectories starting from the same point.\"\"\"\n",
        "    num_steps = 100\n",
        "    initial_time = 0\n",
        "    step_size = 0.1\n",
        "    initial_sample = 0\n",
        "    num_trajectories = 5\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(num_trajectories):\n",
        "        trajectory, time_values = forward_diffusion_1D(\n",
        "            initial_sample,\n",
        "            constant_noise_strength,\n",
        "            initial_time,\n",
        "            num_steps,\n",
        "            step_size\n",
        "        )\n",
        "        plt.plot(time_values, trajectory)\n",
        "\n",
        "    plt.xlabel('Time', fontsize=14)\n",
        "    plt.ylabel('Sample Value', fontsize=14)\n",
        "    plt.title('Forward Diffusion Process - Multiple Trajectories', fontsize=16)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "921TCcg9yeEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reverse Diffusion\n",
        "\n",
        "The reverse diffusion process allows us to generate new data by starting from noise and\n",
        "progressively denoising it. The reverse process is given by:\n",
        "\n",
        "x(t + Δt) = x(t) + σ(T-t)² · ∇_x log p(x, T-t) · Δt + σ(T-t) · √(Δt) · ε\n",
        "\n",
        "The function ∇_x log p(x, t) is called the \"score function\" and represents the gradient of the\n",
        "log probability density. If we can learn this score function, we can reverse the diffusion process\n",
        "to generate data from noise.\n",
        "\n",
        "For the special case where our initial distribution is concentrated at x₀ = 0 and the noise strength\n",
        "is constant, the score function has the closed form:\n",
        "\n",
        "s(x, t) = -x/(σ² · t)"
      ],
      "metadata": {
        "id": "Q4OWnuZFygWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Score function for the special case\n",
        "def simple_score_function(x, x0, noise_strength, t):\n",
        "    \"\"\"\n",
        "    Score function for the special case where the initial distribution\n",
        "    is concentrated at x0 and noise strength is constant.\n",
        "\n",
        "    Args:\n",
        "        x: Current sample value\n",
        "        x0: Initial distribution center (typically 0)\n",
        "        noise_strength: Current noise strength\n",
        "        t: Current time\n",
        "\n",
        "    Returns:\n",
        "        Score value\n",
        "    \"\"\"\n",
        "    score = score = - (x-x0)/((noise_strength**2)*t)\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "7y2B_yJwymMA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Implement the Reverse Diffusion Process"
      ],
      "metadata": {
        "id": "j-jIOreCyoqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the reverse diffusion process\n",
        "def reverse_diffusion_1D(initial_sample, noise_strength_fn, score_fn, final_time, num_steps, step_size):\n",
        "    \"\"\"\n",
        "    Simulates the reverse diffusion process in 1D.\n",
        "\n",
        "    Args:\n",
        "        initial_sample: Starting value (scalar, typically sampled from noise distribution)\n",
        "        noise_strength_fn: Function that returns noise strength at given time\n",
        "        score_fn: Score function that guides the reverse process\n",
        "        final_time: Total diffusion time (T)\n",
        "        num_steps: Number of reverse steps to simulate\n",
        "        step_size: Size of each time step\n",
        "\n",
        "    Returns:\n",
        "        tuple: (trajectory array, time array)\n",
        "    \"\"\"\n",
        "    # Initialize arrays to store the trajectory and time values\n",
        "    trajectory = np.zeros(num_steps + 1); trajectory[0] = initial_sample\n",
        "    time_values = np.arange(num_steps + 1)*step_size\n",
        "\n",
        "    for i in range(num_steps):\n",
        "      #   1. Calculate the reverse time (final_time - current_time)\n",
        "      reverse_time = final_time - time_values[i]\n",
        "      #   2. Get the current noise strength\n",
        "      noise_strength = noise_strength_fn(reverse_time)\n",
        "      #   3. Calculate the score value using the score function\n",
        "      score = score_fn(time_values[i], 0, noise_strength, final_time-time_values[i])\n",
        "\n",
        "      ############# YOUR CODE HERE (2 lines) Perform reverse diffusion steps\n",
        "      #   TODO: 4. Generate random noise\n",
        "      #   TODO: 5. Update the trajectory according to the reverse diffusion equation\n",
        "      #############\n",
        "\n",
        "    return trajectory, time_values\n",
        "\n",
        "# Test the reverse diffusion function\n",
        "def visualize_reverse_diffusion():\n",
        "    \"\"\"Visualize multiple reverse diffusion trajectories starting from noise.\"\"\"\n",
        "    num_steps = 100\n",
        "    step_size = 0.1\n",
        "    final_time = 11\n",
        "    num_trajectories = 5\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(num_trajectories):\n",
        "        # Start from a noisy point (sample from the noise distribution)\n",
        "        noisy_sample = np.random.normal(loc=0, scale=final_time)\n",
        "\n",
        "        trajectory, time_values = reverse_diffusion_1D(\n",
        "            noisy_sample,\n",
        "            constant_noise_strength,\n",
        "            simple_score_function,\n",
        "            final_time,\n",
        "            num_steps,\n",
        "            step_size\n",
        "        )\n",
        "        plt.plot(time_values, trajectory)\n",
        "\n",
        "    plt.xlabel('Time', fontsize=14)\n",
        "    plt.ylabel('Sample Value', fontsize=14)\n",
        "    plt.title('Reverse Diffusion Process - Multiple Trajectories', fontsize=16)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gz0aCUHhywm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural Network Architecture for Image Diffusion\n",
        "\n",
        "To effectively apply diffusion models to images, we need neural network architectures that work well\n",
        "with spatial data. The U-Net architecture is an excellent choice because it:\n",
        "\n",
        "1. Maintains spatial information through skip connections\n",
        "2. Processes images at multiple resolutions via downsampling and upsampling\n",
        "3. Can effectively capture both local and global image features\n",
        "\n",
        "Since our score function must be time-dependent, we also need a way to condition our network on time.\n",
        "We'll use sinusoidal time embeddings that convert a scalar time value into a high-dimensional vector.\n",
        "\n",
        "### Task 2.1: Implement Time Embedding Modules"
      ],
      "metadata": {
        "id": "Xi-vBgzly0IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time embedding modules\n",
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    Gaussian random features for encoding time steps using sinusoidal functions.\n",
        "    This creates a high-dimensional representation of time that helps the model\n",
        "    distinguish between different diffusion timesteps.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, scale=30.0):\n",
        "        super().__init__()\n",
        "        # Randomly sample frequencies during initialization\n",
        "        # These frequencies are fixed during optimization\n",
        "        self.weight = nn.Parameter(torch.randn(embedding_dim // 2) * scale, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Time values [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            Time embeddings [batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    A dense layer that reshapes time embeddings for injection into convolutional layers.\n",
        "    This allows time information to be added directly to feature maps.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Time embeddings [batch_size, input_dim]\n",
        "\n",
        "        Returns:\n",
        "            Reshaped time embeddings [batch_size, output_dim, 1, 1]\n",
        "            which can be broadcast-added to convolutional feature maps\n",
        "        \"\"\"\n",
        "        # TODO: Apply the dense layer and reshape the output for broadcasting to feature maps\n",
        "        # The output should have shape [batch_size, output_dim, 1, 1]\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "a4Uz7L0By537"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Implement the U-Net Architecture for Diffusion Models"
      ],
      "metadata": {
        "id": "2iOT16ZMzBKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for image-based diffusion models with time conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_distribution_fn, channels=[32, 64, 128, 256], embed_dim=256):\n",
        "        \"\"\"\n",
        "        Initialize a time-dependent score-based network based on U-Net architecture.\n",
        "\n",
        "        Args:\n",
        "            noise_distribution_fn: Function that maps time t to the standard deviation\n",
        "                                  of the perturbation kernel p(x(t)|x(0))\n",
        "            channels: Number of channels for feature maps at each resolution\n",
        "            embed_dim: Dimensionality of time embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding layers\n",
        "        self.time_embed = nn.Sequential(\n",
        "            GaussianFourierProjection(embed_dim=embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.conv_in = nn.Conv2d(1, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Create a TimeEmbedding layer to inject time information\n",
        "        self.time_embed1 = TimeEmbedding(embed_dim, channels[0])\n",
        "        self.norm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size=3, stride=2, bias=False)\n",
        "        self.time_embed2 = TimeEmbedding(embed_dim, channels[1])\n",
        "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "        self.dense3 = TimeEmbedding(embed_dim, channels[2])\n",
        "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "\n",
        "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "        self.dense4 = TimeEmbedding(embed_dim, channels[3])\n",
        "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
        "\n",
        "        # TODO: Define the decoding (upsampling) path with skip connections\n",
        "        # For each level in the upsampling path:\n",
        "        # 1. Create a ConvTranspose2d layer\n",
        "        # 2. Create a TimeEmbedding layer\n",
        "        # 3. Add normalization\n",
        "\n",
        "        # YOUR CODE HERE - Decoding Path\n",
        "\n",
        "        # TODO: Define the output layer\n",
        "\n",
        "        # Activation function: Swish/SiLU\n",
        "        self.activation = nn.SiLU()\n",
        "\n",
        "        # Store noise function for normalization during inference\n",
        "        self.noise_distribution_fn = noise_distribution_fn\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the U-Net.\n",
        "\n",
        "        Args:\n",
        "            x: Input images [batch_size, 1, height, width]\n",
        "            t: Diffusion timesteps [batch_size]\n",
        "            class_labels: Optional class conditioning (not used in base model)\n",
        "\n",
        "        Returns:\n",
        "            Predicted score (noise estimate) [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass through the U-Net\n",
        "        # 1. Get time embeddings\n",
        "        # 2. Pass through the encoding path\n",
        "        # 3. Pass through the decoding path with skip connections\n",
        "        # 4. Apply the output layer\n",
        "        # 5. Normalize the output by the noise level\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Obtain the Gaussian random feature embedding for t\n",
        "        temb = self.activation(self.time_embed(t))\n",
        "\n",
        "        # TODO: Implement encoding path (downsampling)\n",
        "        h1 = None  # Replace with your implementation\n",
        "        h2 = None  # Replace with your implementation\n",
        "        h3 = None  # Replace with your implementation\n",
        "        h4 = None  # Replace with your implementation\n",
        "\n",
        "        # TODO: Implement decoding path (upsampling) with skip connections\n",
        "        h = None  # Replace with your implementation\n",
        "\n",
        "        # TODO: Normalize output\n",
        "        return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "B3TCLJ0NzR4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Implement a Residual U-Net Variant (Optional - for bonus points)\n",
        "\n",
        "For advanced students: implement a variant of the U-Net with residual connections\n",
        "instead of concatenation for skip connections."
      ],
      "metadata": {
        "id": "STi30iQWzUc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionUNetResidual(nn.Module):\n",
        "    \"\"\"\n",
        "    Alternative U-Net architecture with residual connections instead of concatenation.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_distribution_fn, channels=[32, 64, 128, 256], embed_dim=256):\n",
        "        \"\"\"\n",
        "        Initialize a U-Net with residual skip connections instead of concatenation.\n",
        "\n",
        "        Args:\n",
        "            noise_distribution_fn: Function that maps time t to the standard deviation\n",
        "            channels: Number of channels for feature maps at each resolution\n",
        "            embed_dim: Dimensionality of time embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Implement the residual U-Net architecture\n",
        "        # The structure is similar to the regular U-Net but uses addition\n",
        "        # instead of concatenation for skip connections\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the U-Net with residual connections.\n",
        "\n",
        "        Args:\n",
        "            x: Input images [batch_size, 1, height, width]\n",
        "            t: Diffusion timesteps [batch_size]\n",
        "            class_labels: Optional class conditioning (not used in this model)\n",
        "\n",
        "        Returns:\n",
        "            Predicted score (noise estimate) [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass with residual connections\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "zuTBREfIz5E7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training the Diffusion Model\n",
        "\n",
        "Now we'll define the specific diffusion process and the loss function to train our model.\n",
        "Our forward process follows the stochastic differential equation (SDE):\n",
        "\n",
        "dx = σ^t dw\n",
        "\n",
        "where σ is a noise parameter and w is the Wiener process (Brownian motion).\n",
        "\n",
        "### Task 3.1: Implement the Noise Distribution and Diffusion Functions"
      ],
      "metadata": {
        "id": "5mciyRZs0Bsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the marginal noise distribution\n",
        "def marginal_prob_std(t, sigma=25.0):\n",
        "    \"\"\"\n",
        "    Compute the standard deviation of p_{0t}(x(t) | x(0)).\n",
        "\n",
        "    Args:\n",
        "        t: Time values [batch_size]\n",
        "        sigma: Base noise multiplier\n",
        "\n",
        "    Returns:\n",
        "        Standard deviation at time t\n",
        "    \"\"\"\n",
        "\n",
        "    t = = torch.tensor(t, device=device)\n",
        "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
        "\n",
        "\n",
        "def diffusion_coeff(t, sigma=25.0):\n",
        "    \"\"\"\n",
        "    Compute the diffusion coefficient of the SDE.\n",
        "\n",
        "    Args:\n",
        "        t: Time values [batch_size]\n",
        "        sigma: Base noise multiplier\n",
        "\n",
        "    Returns:\n",
        "        Diffusion coefficient at time t\n",
        "    \"\"\"\n",
        "    return torch.tensor(sigma**t, device=device)\n",
        "\n",
        "# Create functions with default parameter\n",
        "noise_distribution_fn = functools.partial(marginal_prob_std, sigma=25.0)\n",
        "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=25.0)"
      ],
      "metadata": {
        "id": "kJRkYFWAoX_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2: Implement the Diffusion Loss Function"
      ],
      "metadata": {
        "id": "gCcn8af8ohna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the loss function for training the diffusion model\n",
        "def diffusion_loss_fn(model, images, noise_distribution_fn, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Loss function for training score-based generative models.\n",
        "\n",
        "    Args:\n",
        "        model: The score model network\n",
        "        images: Training images [batch_size, 1, height, width]\n",
        "        noise_distribution_fn: Function computing noise std at time t\n",
        "        epsilon: Small value for numerical stability\n",
        "\n",
        "    Returns:\n",
        "        Average loss value for the batch\n",
        "    \"\"\"\n",
        "    # TODO: Implement the diffusion loss function with the following steps:\n",
        "\n",
        "    # 5. Predict the noise component using the model\n",
        "    # 6. Calculate MSE between predicted and actual noise, properly weighted\n",
        "\n",
        "    # 1. Sample random timesteps for each image\n",
        "    random_t = torch.rand(images.shape[0], device=images.device) * (1. - eps) + eps\n",
        "    # 2. Get the noise standard deviation at sampled times\n",
        "    std = marginal_prob_std(random_t)\n",
        "\n",
        "    ##### YOUR CODE HERE\n",
        "    # TODO: 3. Generate random noise with the same shape as input images\n",
        "    z =\n",
        "    # TODO: 4. Add noise to the input images according to the noise schedule\n",
        "\n",
        "    # Sample random timesteps for each image\n",
        "    batch_size = None  # Replace with your implementation\n",
        "    random_t = None  # Replace with your implementation\n",
        "\n",
        "    # Get the noise standard deviation\n",
        "    noise_std = None  # Replace with your implementation\n",
        "\n",
        "    # Generate random noise\n",
        "    noise = None  # Replace with your implementation\n",
        "\n",
        "    # Add noise to the images\n",
        "    noisy_images = None  # Replace with your implementation\n",
        "\n",
        "    # Predict the noise component\n",
        "    predicted_noise = None\n",
        "\n",
        "    # Calculate weighted MSE loss\n",
        "    loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1,2,3)))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "QpT_NOdjpgr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.3: Implement the Conditional Loss Function"
      ],
      "metadata": {
        "id": "qxuR3d16poo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the conditional loss function for class-conditioned generation\n",
        "def conditional_diffusion_loss_fn(model, images, class_labels, noise_distribution_fn, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Loss function for training class-conditioned diffusion models.\n",
        "\n",
        "    Args:\n",
        "        model: The score model network\n",
        "        images: Training images [batch_size, 1, height, width]\n",
        "        class_labels: Class labels for conditioning [batch_size]\n",
        "        noise_distribution_fn: Function computing noise std at time t\n",
        "        epsilon: Small value for numerical stability\n",
        "\n",
        "    Returns:\n",
        "        Average loss value for the batch\n",
        "    \"\"\"\n",
        "    # TODO: Implement the conditional diffusion loss function\n",
        "    # This is similar to the unconditional loss, but passes class_labels to the model\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "2VR_UvZdtp9d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.4: Implement the Euler-Maruyama Sampler"
      ],
      "metadata": {
        "id": "e1Z8I_Astr34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euler_maruyama_sampler(\n",
        "    score_model,\n",
        "    noise_distribution_fn,\n",
        "    diffusion_coeff_fn,\n",
        "    batch_size=64,\n",
        "    img_shape=(1, 28, 28),\n",
        "    num_steps=500,\n",
        "    device=\"cuda\",\n",
        "    epsilon=1e-3,\n",
        "    class_labels=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate samples from the score-based model using Euler-Maruyama solver.\n",
        "\n",
        "    Args:\n",
        "        score_model: The trained score model\n",
        "        noise_distribution_fn: Function mapping time to noise std\n",
        "        diffusion_coeff_fn: Function mapping time to diffusion coefficient\n",
        "        batch_size: Number of samples to generate\n",
        "        img_shape: Shape of a single image\n",
        "        num_steps: Number of sampling steps\n",
        "        device: Device to run on ('cuda' or 'cpu')\n",
        "        epsilon: Small time value for numerical stability\n",
        "        class_labels: Optional class labels for conditional generation\n",
        "\n",
        "    Returns:\n",
        "        Generated samples [batch_size, channels, height, width]\n",
        "    \"\"\"\n",
        "\n",
        "    # Start from pure noise\n",
        "    t = torch.ones(batch_size, device=device)\n",
        "    init_noise = torch.randn(batch_size, *img_shape, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
        "\n",
        "    # Setup time steps\n",
        "    time_steps = torch.linspace(1., epsilon, num_steps, device=device)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "\n",
        "    # Progressive denoising\n",
        "    x = init_noise\n",
        "    with torch.no_grad():\n",
        "        for time_step in tqdm(time_steps):\n",
        "          batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
        "          g = diffusion_coeff(batch_time_step)\n",
        "          mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step, y=class_labels) * step_size\n",
        "          x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Utility function to save generated samples\n",
        "def save_samples(samples, filename):\n",
        "    \"\"\"\n",
        "    Save generated samples as an image.\n",
        "\n",
        "    Args:\n",
        "        samples: Generated samples [batch_size, channels, height, width]\n",
        "        filename: Path to save the image\n",
        "    \"\"\"\n",
        "    # Ensure pixel values are in valid range\n",
        "    samples = samples.clamp(0.0, 1.0)\n",
        "\n",
        "    # Create a grid of images\n",
        "    sample_grid = make_grid(samples, nrow=int(np.sqrt(samples.shape[0])))\n",
        "\n",
        "    # Convert to numpy and transpose dimensions for matplotlib\n",
        "    sample_np = sample_grid.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Save using matplotlib\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(sample_np, vmin=0., vmax=1.)\n",
        "    plt.savefig(filename, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    return sample_np  # Return for display"
      ],
      "metadata": {
        "id": "OXRVAmNuvbOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Utility Functions\n",
        "\n",
        "Below are utility functions for training diffusion models and saving/loading checkpoints.\n",
        "These functions include proper checkpointing to allow resuming training sessions.\n",
        "\n",
        "### Task 4.1: Implement Model Training Function with Checkpointing"
      ],
      "metadata": {
        "id": "mb_nysoCvfbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the diffusion model training function\n",
        "def train_diffusion_model(\n",
        "    model,\n",
        "    dataset,\n",
        "    noise_distribution_fn,\n",
        "    num_epochs=50,\n",
        "    batch_size=256,\n",
        "    learning_rate=1e-4,\n",
        "    conditional=False,\n",
        "    checkpoint_freq=5,\n",
        "    checkpoint_path=None,  # Changed from a default string to None\n",
        "    resume_training=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a diffusion model with checkpointing.\n",
        "\n",
        "    Args:\n",
        "        model: The diffusion model to train\n",
        "        dataset: Dataset for training\n",
        "        noise_distribution_fn: Function to compute noise std at different times\n",
        "        num_epochs: Number of epochs to train for\n",
        "        batch_size: Batch size for training\n",
        "        learning_rate: Learning rate for the optimizer\n",
        "        conditional: Whether the model is class-conditional\n",
        "        checkpoint_freq: How often to save checkpoints (in epochs)\n",
        "        checkpoint_path: Path to save/load model checkpoints. If None, uses CHECKPOINT_DIR.\n",
        "        resume_training: Whether to resume from a checkpoint\n",
        "\n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "    # Set default checkpoint path if not provided\n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_filename = \"conditional_diffusion.pt\" if conditional else \"diffusion_model.pt\"\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint_filename)\n",
        "\n",
        "    # TODO: Implement the training function with these steps:\n",
        "    # 1. Setup data loader\n",
        "    # 2. Initialize optimizer and learning rate scheduler\n",
        "    # 3. Initialize tracking variables\n",
        "    # 4. Resume from checkpoint if requested\n",
        "    # 5. Implement the training loop:\n",
        "    #    a. Process batches and compute loss (conditional or not)\n",
        "    #    b. Backpropagation and optimization\n",
        "    #    c. Track statistics and update progress\n",
        "    # 6. Save checkpoints periodically\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    # Setup data loader\n",
        "    data_loader = None  # Replace with your implementation\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = None  # Replace with your implementation\n",
        "    scheduler = None  # Replace with your implementation\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    start_epoch = 0\n",
        "    training_history = []\n",
        "\n",
        "    # Resume from checkpoint if requested\n",
        "    if resume_training and os.path.exists(checkpoint_path):\n",
        "        # TODO: Implement checkpoint loading\n",
        "        pass\n",
        "\n",
        "    # Create full path for checkpoint (parent directories)\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(checkpoint_path)), exist_ok=True)\n",
        "\n",
        "    # Setup training loop\n",
        "    model.train()\n",
        "    progress_bar = trange(start_epoch, num_epochs)\n",
        "\n",
        "    for epoch in progress_bar:\n",
        "        # TODO: Implement epoch loop\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        # TODO: Process batches\n",
        "\n",
        "        # TODO: Update learning rate\n",
        "\n",
        "        # TODO: Calculate average loss and update history\n",
        "\n",
        "        # TODO: Save checkpoint periodically and at the end\n",
        "        # When implementing the checkpoint saving, use the checkpoint_path variable\n",
        "        # This ensures compatibility with Google Drive storage when enabled\n",
        "\n",
        "    # Final model saving\n",
        "    model.eval()\n",
        "    return model, training_history"
      ],
      "metadata": {
        "id": "uYqer636vwbE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.2: Implement Model Loading Function"
      ],
      "metadata": {
        "id": "16DIMMKbvxMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the function to load a model from a checkpoint\n",
        "def load_model_from_checkpoint(\n",
        "    model_class,\n",
        "    checkpoint_path,\n",
        "    noise_distribution_fn,\n",
        "    map_location=None,\n",
        "    **model_kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Load a diffusion model from a checkpoint.\n",
        "\n",
        "    Args:\n",
        "        model_class: The model class to instantiate\n",
        "        checkpoint_path: Path to the checkpoint file\n",
        "        noise_distribution_fn: Function to compute noise distribution\n",
        "        map_location: Device mapping for loading the checkpoint\n",
        "        **model_kwargs: Additional arguments for the model constructor\n",
        "\n",
        "    Returns:\n",
        "        Loaded model and training history\n",
        "    \"\"\"\n",
        "    # TODO: Implement the model loading function with these steps:\n",
        "    # 1. Set map_location to the appropriate device\n",
        "    # 2. Check if checkpoint exists\n",
        "    # 3. Load the checkpoint\n",
        "    # 4. Create a model with the same architecture\n",
        "    # 5. Handle parallel models if needed\n",
        "    # 6. Load model weights\n",
        "    # 7. Return the model and training history\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    if map_location is None:\n",
        "        map_location = device\n",
        "\n",
        "    # Check if checkpoint exists\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
        "\n",
        "    # TODO: Load checkpoint and model\n",
        "    checkpoint = None  # Replace with your implementation\n",
        "    model = None  # Replace with your implementation\n",
        "\n",
        "    # TODO: Handle parallel models and load weights\n",
        "\n",
        "    return model, checkpoint.get('history', [])"
      ],
      "metadata": {
        "id": "r07P1YPXwYKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Attention Mechanisms for Conditional Generation\n",
        "\n",
        "To enable conditional image generation (generating a specific digit based on a text prompt),\n",
        "we'll enhance our model with attention mechanisms. Attention allows the model to focus on\n",
        "relevant parts of the input when making predictions, and helps bridge the gap between\n",
        "text and image modalities.\n",
        "\n",
        "We'll implement:\n",
        "1. Cross-Attention: To make the image features attend to text features\n",
        "2. Self-Attention: To make each part of the image aware of other parts\n",
        "3. Transformer blocks: Combining attention with feed-forward networks\n",
        "\n",
        "### Task 5.1: Implement the Text Embedding Layer"
      ],
      "metadata": {
        "id": "4qJxab3vwZhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement word embedding for digit labels (0-9)\n",
        "class TextEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple word embedding layer to convert digit labels into vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=10, embed_dim=256):\n",
        "        super(TextEmbedding, self).__init__()\n",
        "        # TODO: Initialize an embedding layer for the vocabulary\n",
        "        # Add +1 for padding token if needed\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.embedding = None  # Replace with your implementation\n",
        "\n",
        "    def forward(self, text_indices):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text_indices: Integer indices representing text tokens [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            Embedded vectors [batch_size, embed_dim]\n",
        "        \"\"\"\n",
        "        # TODO: Apply the embedding layer to the input indices\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "-hLEVa1Mwiyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.2: Implement the Cross-Attention Mechanism"
      ],
      "metadata": {
        "id": "LCOPiDWZwldW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the cross-attention layer\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Cross-attention layer that allows image features to attend to text features.\n",
        "    \"\"\"\n",
        "    def __init__(self, query_dim, key_dim=None, embed_dim=256, num_heads=1):\n",
        "        \"\"\"\n",
        "        Initialize cross-attention module.\n",
        "\n",
        "        Args:\n",
        "            query_dim: Dimension of query vectors\n",
        "            key_dim: Dimension of key/value vectors (if None, same as query_dim)\n",
        "            embed_dim: Output embedding dimension\n",
        "            num_heads: Number of attention heads (simplified to 1 for clarity)\n",
        "        \"\"\"\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query_dim = query_dim\n",
        "        self.key_dim = key_dim if key_dim is not None else query_dim\n",
        "\n",
        "        # TODO: Initialize linear projections for Q, K, V\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.to_q = None  # Replace with your implementation\n",
        "        self.to_k = None  # Replace with your implementation\n",
        "        self.to_v = None  # Replace with your implementation\n",
        "\n",
        "        self.is_self_attention = key_dim is None\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        Apply attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            x: Query tensor [batch_size, seq_len_q, query_dim]\n",
        "            context: Key/value tensor [batch_size, seq_len_kv, key_dim]\n",
        "                    (if None, uses self-attention)\n",
        "\n",
        "        Returns:\n",
        "            Attended features [batch_size, seq_len_q, query_dim]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the attention mechanism with these steps:\n",
        "        # 1. Handle self-attention vs cross-attention cases\n",
        "        # 2. Project inputs to query, key, and value\n",
        "        # 3. Calculate attention scores (dot product of query and key)\n",
        "        # 4. Scale scores and apply softmax to get attention weights\n",
        "        # 5. Apply attention weights to values\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Handle self-attention vs cross-attention\n",
        "        q = None  # Replace with your implementation\n",
        "        k = None  # Replace with your implementation\n",
        "        v = None  # Replace with your implementation\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attn_scores = None  # Replace with your implementation\n",
        "\n",
        "        # Scale scores and apply softmax\n",
        "        attn_weights = None  # Replace with your implementation\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        output = None  # Replace with your implementation\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "RXhwkCO9wpim"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.3: Implement the Transformer Block"
      ],
      "metadata": {
        "id": "MQpna1OfwvqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a transformer block combining self-attention, cross-attention, and feed-forward networks\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer block combining self-attention, cross-attention, and feed-forward layers.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim, context_dim=None):\n",
        "        \"\"\"\n",
        "        Initialize a transformer block.\n",
        "\n",
        "        Args:\n",
        "            feature_dim: Dimension of input features\n",
        "            context_dim: Dimension of context features for cross-attention\n",
        "        \"\"\"\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        # TODO: Initialize transformer components:\n",
        "        # 1. Self-attention layer\n",
        "        # 2. Cross-attention layer (if context_dim is provided)\n",
        "        # 3. Normalization layers\n",
        "        # 4. Feed-forward network\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.self_attn = None  # Replace with your implementation\n",
        "        self.cross_attn = None  # Replace with your implementation\n",
        "\n",
        "        self.norm1 = None  # Replace with your implementation\n",
        "        self.norm2 = None  # Replace with your implementation\n",
        "        self.norm3 = None  # Replace with your implementation\n",
        "\n",
        "        self.ff_network = None  # Replace with your implementation\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        Apply transformer block.\n",
        "\n",
        "        Args:\n",
        "            x: Input features [batch_size, seq_len, feature_dim]\n",
        "            context: Optional context features for cross-attention\n",
        "                    [batch_size, context_len, context_dim]\n",
        "\n",
        "        Returns:\n",
        "            Transformed features [batch_size, seq_len, feature_dim]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the transformer block forward pass:\n",
        "        # 1. Apply self-attention with residual connection\n",
        "        # 2. Apply cross-attention with residual connection (if context is provided)\n",
        "        # 3. Apply feed-forward network with residual connection\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "7nhd76bQxKOx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.4: Implement the Spatial Transformer for 2D Feature Maps"
      ],
      "metadata": {
        "id": "sopclwgnxMP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a transformer for spatial (image) data\n",
        "class SpatialTransformer(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer module for spatial data (images) that reshapes 2D feature maps\n",
        "    into sequences, applies transformer operations, and reshapes back.\n",
        "    \"\"\"\n",
        "    def __init__(self, feature_dim, context_dim=None):\n",
        "        \"\"\"\n",
        "        Initialize spatial transformer.\n",
        "\n",
        "        Args:\n",
        "            feature_dim: Number of feature channels\n",
        "            context_dim: Dimension of context features\n",
        "        \"\"\"\n",
        "        super(SpatialTransformer, self).__init__()\n",
        "\n",
        "        # TODO: Initialize a transformer block\n",
        "        self.transformer = None  # Replace with your implementation\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        Apply spatial transformer to image features.\n",
        "\n",
        "        Args:\n",
        "            x: Image features [batch_size, channels, height, width]\n",
        "            context: Context features [batch_size, context_len, context_dim]\n",
        "\n",
        "        Returns:\n",
        "            Transformed image features [batch_size, channels, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement spatial transformer forward pass:\n",
        "        # 1. Save input and extract dimensions\n",
        "        # 2. Reshape spatial dimensions into sequence length\n",
        "        # 3. Apply transformer block\n",
        "        # 4. Reshape back to spatial representation\n",
        "        # 5. Add residual connection\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Save input and extract dimensions\n",
        "        orig_x = x\n",
        "        batch, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape spatial dimensions into sequence length\n",
        "        # [batch, channels, height, width] -> [batch, height*width, channels]\n",
        "        x = None  # Replace with your implementation\n",
        "\n",
        "        # Apply transformer\n",
        "        x = None  # Replace with your implementation\n",
        "\n",
        "        # Reshape back to spatial representation\n",
        "        # [batch, height*width, channels] -> [batch, channels, height, width]\n",
        "        x = None  # Replace with your implementation\n",
        "\n",
        "        # Add residual connection\n",
        "        return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "CrzUJX0nxQii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.5: Implement the Conditional Diffusion U-Net Model"
      ],
      "metadata": {
        "id": "zw7TuKuDxTSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a U-Net with cross-attention for text-conditioned image generation\n",
        "class ConditionalDiffusionUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net model with cross-attention for text-conditioned image generation.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_distribution_fn, channels=[32, 64, 128, 256],\n",
        "                 embed_dim=256, text_dim=256, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initialize conditional U-Net model with attention mechanisms.\n",
        "\n",
        "        Args:\n",
        "            noise_distribution_fn: Function that maps time t to noise standard deviation\n",
        "            channels: List of channel dimensions for each resolution level\n",
        "            embed_dim: Dimension of time embeddings\n",
        "            text_dim: Dimension of text embeddings\n",
        "            num_classes: Number of possible classes (digits 0-9 for MNIST)\n",
        "        \"\"\"\n",
        "        super(ConditionalDiffusionUNet, self).__init__()\n",
        "\n",
        "        # TODO: Initialize model components:\n",
        "        # 1. Time embedding\n",
        "        # 2. Text embedding\n",
        "        # 3. Encoder (downsampling) path with attention blocks\n",
        "        # 4. Decoder (upsampling) path with attention blocks and skip connections\n",
        "        # 5. Output layer\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = None  # Replace with your implementation\n",
        "\n",
        "        # Text embedding for digit conditioning\n",
        "        self.text_embedding = None  # Replace with your implementation\n",
        "\n",
        "        # TODO: Implement encoding (downsampling) path\n",
        "\n",
        "        # TODO: Implement decoding (upsampling) path with skip connections\n",
        "\n",
        "        # Activation function\n",
        "        self.activation = nn.SiLU()\n",
        "\n",
        "        # Store noise function for normalization\n",
        "        self.noise_distribution_fn = noise_distribution_fn\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass including text conditioning via cross-attention.\n",
        "\n",
        "        Args:\n",
        "            x: Input noisy images [batch_size, 1, height, width]\n",
        "            t: Diffusion timesteps [batch_size]\n",
        "            class_labels: Class indices for conditioning [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            Predicted score (noise estimate) [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the conditional forward pass with these steps:\n",
        "        # 1. Get time embedding\n",
        "        # 2. Get text conditioning\n",
        "        # 3. Apply encoding path with attention to text features\n",
        "        # 4. Apply decoding path with attention and skip connections\n",
        "        # 5. Generate output and normalize by noise level\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Get time embedding\n",
        "        time_emb = None  # Replace with your implementation\n",
        "\n",
        "        # Get text conditioning\n",
        "        text_emb = None  # Replace with your implementation\n",
        "\n",
        "        # TODO: Implement encoding path with attention\n",
        "\n",
        "        # TODO: Implement decoding path with attention and skip connections\n",
        "\n",
        "        # TODO: Generate and normalize output\n",
        "\n",
        "        return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "udxS2YY2xgMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Latent Diffusion with Autoencoders\n",
        "\n",
        "Instead of performing diffusion directly in pixel space, we can operate in a compressed latent space.\n",
        "This approach, introduced in the Stable Diffusion paper, has several advantages:\n",
        "\n",
        "1. Computational efficiency: Working with smaller latent representations is faster\n",
        "2. Better modeling: The latent space may capture more semantic information\n",
        "3. Better quality: Can lead to higher quality generation\n",
        "\n",
        "We'll implement an autoencoder to compress MNIST images to a latent space, then\n",
        "perform diffusion in this compressed representation.\n",
        "\n",
        "### Task 6.1: Implement the Autoencoder Architecture"
      ],
      "metadata": {
        "id": "-Qx0cf6Mxjht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the autoencoder for latent space compression\n",
        "class Autoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder for compressing images to a latent representation and reconstructing them.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels=[8, 16, 32]):\n",
        "        \"\"\"\n",
        "        Initialize autoencoder with encoder and decoder components.\n",
        "\n",
        "        Args:\n",
        "            channels: Channel dimensions for feature maps at different levels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Implement encoder network to compress image to latent representation\n",
        "        # The encoder should contain:\n",
        "        # 1. Multiple convolutional layers with downsampling\n",
        "        # 2. Batch normalization\n",
        "        # 3. Activation functions\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.encoder = None  # Replace with your implementation\n",
        "\n",
        "        # TODO: Implement decoder network to reconstruct image from latent representation\n",
        "        # The decoder should contain:\n",
        "        # 1. Multiple transpose convolutional layers with upsampling\n",
        "        # 2. Batch normalization\n",
        "        # 3. Activation functions\n",
        "        # 4. Final output activation (e.g., sigmoid for 0-1 normalized images)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        self.decoder = None  # Replace with your implementation\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Full autoencoder forward pass: encode then decode.\n",
        "\n",
        "        Args:\n",
        "            x: Input images [batch_size, 1, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Reconstructed images [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the autoencoder forward pass:\n",
        "        # 1. Encode input to latent representation\n",
        "        # 2. Decode latent back to image\n",
        "        # 3. Handle any size mismatch (due to transpose convolutions)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Encode input to latent representation\n",
        "        latent = None  # Replace with your implementation\n",
        "\n",
        "        # Decode latent back to image\n",
        "        reconstructed = None  # Replace with your implementation\n",
        "\n",
        "        # Handle potential size mismatch\n",
        "        if reconstructed.shape != x.shape:\n",
        "            # TODO: Implement center cropping to match original size\n",
        "            pass\n",
        "\n",
        "        return reconstructed\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input to latent representation.\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, latent):\n",
        "        \"\"\"Decode latent representation to image.\"\"\"\n",
        "        return self.decoder(latent)"
      ],
      "metadata": {
        "id": "qOvOn-yCxp0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6.2: Implement the Latent Diffusion Model"
      ],
      "metadata": {
        "id": "fSDWboklxsXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the latent diffusion model\n",
        "class LatentDiffusionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Diffusion model operating in the latent space of an autoencoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, autoencoder, noise_distribution_fn,\n",
        "                 latent_channels=32, hidden_channels=[64, 128, 256],\n",
        "                 embed_dim=256, text_dim=256, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initialize latent diffusion model.\n",
        "\n",
        "        Args:\n",
        "            autoencoder: Trained autoencoder model\n",
        "            noise_distribution_fn: Function mapping time to noise std\n",
        "            latent_channels: Number of channels in autoencoder latent space\n",
        "            hidden_channels: Channel dimensions for U-Net layers\n",
        "            embed_dim: Dimension of time embeddings\n",
        "            text_dim: Dimension of text/class embeddings\n",
        "            num_classes: Number of possible conditioning classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Initialize the latent diffusion model:\n",
        "        # 1. Store the autoencoder (frozen)\n",
        "        # 2. Create a conditional diffusion U-Net for the latent space\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Store autoencoder (should be pre-trained and frozen)\n",
        "        self.autoencoder = autoencoder\n",
        "        # TODO: Freeze the autoencoder parameters\n",
        "\n",
        "        # Complete channel specification for U-Net\n",
        "        channels = [latent_channels] + hidden_channels\n",
        "\n",
        "        # Use conditional U-Net for latent diffusion\n",
        "        self.diffusion_model = None  # Replace with your implementation\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass for training.\n",
        "\n",
        "        Args:\n",
        "            x: Input latent representations [batch_size, latent_channels, h, w]\n",
        "            t: Diffusion timesteps [batch_size]\n",
        "            class_labels: Optional conditioning labels [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            Predicted score/noise\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass (just using the U-Net on latent inputs)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return None  # Replace with your implementation\n",
        "\n",
        "    def encode_images(self, images):\n",
        "        \"\"\"\n",
        "        Encode images to latent representations.\n",
        "\n",
        "        Args:\n",
        "            images: Input images [batch_size, 1, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Latent representations [batch_size, latent_channels, h', w']\n",
        "        \"\"\"\n",
        "        # TODO: Implement image encoding using the autoencoder\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return None  # Replace with your implementation\n",
        "\n",
        "    def decode_latents(self, latents):\n",
        "        \"\"\"\n",
        "        Decode latent representations to images.\n",
        "\n",
        "        Args:\n",
        "            latents: Latent representations [batch_size, latent_channels, h', w']\n",
        "\n",
        "        Returns:\n",
        "            Reconstructed images [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement latent decoding using the autoencoder\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        return None  # Replace with your implementation\n",
        "\n",
        "    def generate(self, num_samples, class_labels=None, num_steps=500, latent_shape=None):\n",
        "        \"\"\"\n",
        "        Generate images from noise via latent diffusion.\n",
        "\n",
        "        Args:\n",
        "            num_samples: Number of images to generate\n",
        "            class_labels: Optional conditioning labels [num_samples]\n",
        "            num_steps: Number of diffusion sampling steps\n",
        "            latent_shape: Shape of latent representations\n",
        "\n",
        "        Returns:\n",
        "            Generated images [num_samples, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement image generation with these steps:\n",
        "        # 1. Determine latent shape (if not provided)\n",
        "        # 2. Generate latents using the diffusion model\n",
        "        # 3. Decode latents to images\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Determine latent shape if not provided\n",
        "        if latent_shape is None:\n",
        "            # Default latent shape for MNIST with our encoder\n",
        "            latent_shape = None  # Replace with your implementation\n",
        "\n",
        "        # Generate latents using the diffusion model\n",
        "        latents = None  # Replace with your implementation\n",
        "\n",
        "        # Decode latents to images\n",
        "        images = None  # Replace with your implementation\n",
        "\n",
        "        return images"
      ],
      "metadata": {
        "id": "t6LJ43RAyJot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Putting It All Together: Training and Evaluation\n",
        "\n",
        "In this section, you'll implement functions to train and evaluate your diffusion models.\n"
      ],
      "metadata": {
        "id": "vSQ_EiLuySzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These functions are provided. You can modify them for your needs or create new ones.\n",
        "\n",
        "def train_and_evaluate_basic_model(train_model=False):\n",
        "    \"\"\"Train and evaluate a basic unconditioned diffusion model.\"\"\"\n",
        "    print(\"\\n1. Basic diffusion model (unconditioned)...\\n\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    mnist_train = MNIST('.', train=True, transform=transform, download=True)\n",
        "    mnist_test = MNIST('.', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Model path\n",
        "    model_path = os.path.join(CHECKPOINT_DIR, \"basic_diffusion.pt\")\n",
        "\n",
        "    if train_model:\n",
        "        # Create and train the model\n",
        "        basic_model = torch.nn.DataParallel(DiffusionUNet(noise_distribution_fn=noise_distribution_fn))\n",
        "        basic_model = basic_model.to(device)\n",
        "\n",
        "        print(f\"Training basic diffusion model...\")\n",
        "        basic_model, history = train_diffusion_model(\n",
        "            basic_model,\n",
        "            mnist_train,\n",
        "            noise_distribution_fn,\n",
        "            num_epochs=30,\n",
        "            batch_size=256,\n",
        "            learning_rate=1e-4,\n",
        "            checkpoint_path=model_path,\n",
        "            resume_training=False\n",
        "        )\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([h['epoch'] for h in history], [h['loss'] for h in history])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss')\n",
        "        plt.savefig(os.path.join(CHECKPOINT_DIR, \"basic_model_loss.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    try:\n",
        "        # Load model from checkpoint\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        basic_model, _ = load_model_from_checkpoint(\n",
        "            DiffusionUNet,\n",
        "            model_path,\n",
        "            noise_distribution_fn\n",
        "        )\n",
        "\n",
        "        # Generate samples\n",
        "        print(\"Generating samples...\")\n",
        "        samples = euler_maruyama_sampler(\n",
        "            basic_model,\n",
        "            noise_distribution_fn,\n",
        "            diffusion_coeff_fn,\n",
        "            batch_size=25,\n",
        "            num_steps=200,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Save samples\n",
        "        save_path = os.path.join(CHECKPOINT_DIR, \"basic_model_samples.png\")\n",
        "        save_samples(samples, save_path)\n",
        "        print(f\"Samples saved to {save_path}\")\n",
        "\n",
        "        return basic_model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Model checkpoint not found at {model_path}. Train the model first with train_model=True.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_and_evaluate_conditional_model(train_model=False):\n",
        "    \"\"\"Train and evaluate a conditional diffusion model with attention.\"\"\"\n",
        "    print(\"\\n2. Conditional diffusion model with attention...\\n\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    mnist_train = MNIST('.', train=True, transform=transform, download=True)\n",
        "    mnist_test = MNIST('.', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Model path\n",
        "    model_path = os.path.join(CHECKPOINT_DIR, \"conditional_diffusion.pt\")\n",
        "\n",
        "    if train_model:\n",
        "        # Create and train the model\n",
        "        cond_model = torch.nn.DataParallel(ConditionalDiffusionUNet(\n",
        "            noise_distribution_fn=noise_distribution_fn,\n",
        "            channels=[32, 64, 128, 256],\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        ))\n",
        "        cond_model = cond_model.to(device)\n",
        "\n",
        "        print(f\"Training conditional diffusion model...\")\n",
        "        cond_model, history = train_diffusion_model(\n",
        "            cond_model,\n",
        "            mnist_train,\n",
        "            noise_distribution_fn,\n",
        "            num_epochs=50,\n",
        "            batch_size=256,\n",
        "            learning_rate=1e-4,\n",
        "            conditional=True,\n",
        "            checkpoint_path=model_path,\n",
        "            resume_training=False\n",
        "        )\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([h['epoch'] for h in history], [h['loss'] for h in history])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss (Conditional Model)')\n",
        "        plt.savefig(os.path.join(CHECKPOINT_DIR, \"conditional_model_loss.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    try:\n",
        "        # Load model from checkpoint\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        cond_model, _ = load_model_from_checkpoint(\n",
        "            ConditionalDiffusionUNet,\n",
        "            model_path,\n",
        "            noise_distribution_fn,\n",
        "            channels=[32, 64, 128, 256],\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        )\n",
        "\n",
        "        # Generate samples for each digit\n",
        "        for digit in range(10):\n",
        "            print(f\"Generating samples for digit {digit}...\")\n",
        "\n",
        "            # Create labels tensor for the digit\n",
        "            labels = torch.ones(25, dtype=torch.long, device=device) * digit\n",
        "\n",
        "            # Generate samples\n",
        "            samples = euler_maruyama_sampler(\n",
        "                cond_model,\n",
        "                noise_distribution_fn,\n",
        "                diffusion_coeff_fn,\n",
        "                batch_size=25,\n",
        "                num_steps=200,\n",
        "                device=device,\n",
        "                class_labels=labels\n",
        "            )\n",
        "\n",
        "            # Save samples\n",
        "            save_path = os.path.join(CHECKPOINT_DIR, f\"conditional_model_samples_digit{digit}.png\")\n",
        "            save_samples(samples, save_path)\n",
        "            print(f\"Samples for digit {digit} saved to {save_path}\")\n",
        "\n",
        "        return cond_model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Model checkpoint not found at {model_path}. Train the model first with train_model=True.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_and_evaluate_latent_model(train_model=False, train_autoencoder=False):\n",
        "    \"\"\"Train and evaluate a latent diffusion model.\"\"\"\n",
        "    print(\"\\n3. Latent diffusion model with autoencoder...\\n\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    mnist_train = MNIST('.', train=True, transform=transform, download=True)\n",
        "    mnist_test = MNIST('.', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Paths for saving models\n",
        "    ae_path = os.path.join(CHECKPOINT_DIR, \"autoencoder.pt\")\n",
        "    latent_model_path = os.path.join(CHECKPOINT_DIR, \"latent_diffusion.pt\")\n",
        "\n",
        "    # First train or load the autoencoder\n",
        "    autoencoder = Autoencoder(channels=[8, 16, 32]).to(device)\n",
        "\n",
        "    if train_autoencoder:\n",
        "        # Define loss function (MSE) and optimizer\n",
        "        print(\"Training autoencoder...\")\n",
        "        mse_loss = nn.MSELoss()\n",
        "        optimizer = Adam(autoencoder.parameters(), lr=1e-3)\n",
        "\n",
        "        # Setup data loader\n",
        "        data_loader = DataLoader(mnist_train, batch_size=256, shuffle=True, num_workers=4)\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 20\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            num_samples = 0\n",
        "\n",
        "            # Process batches\n",
        "            batch_progress = tqdm(data_loader, desc=f\"AE Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "            for x, _ in batch_progress:\n",
        "                x = x.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                x_recon = autoencoder(x)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = mse_loss(x_recon, x)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Track statistics\n",
        "                epoch_loss += loss.item() * x.shape[0]\n",
        "                num_samples += x.shape[0]\n",
        "\n",
        "                # Update batch progress\n",
        "                batch_progress.set_postfix({\"Batch Loss\": f\"{loss.item():.6f}\"})\n",
        "\n",
        "            # Print epoch results\n",
        "            avg_loss = epoch_loss / num_samples\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # Save trained autoencoder\n",
        "        torch.save(autoencoder.state_dict(), ae_path)\n",
        "        print(f\"Autoencoder saved to {ae_path}\")\n",
        "\n",
        "        # Visualize reconstructions\n",
        "        with torch.no_grad():\n",
        "            test_samples = next(iter(DataLoader(mnist_test, batch_size=16)))[0].to(device)\n",
        "            reconstructions = autoencoder(test_samples)\n",
        "\n",
        "            # Create comparison grid\n",
        "            comparison = torch.cat([test_samples, reconstructions])\n",
        "            save_path = os.path.join(CHECKPOINT_DIR, \"autoencoder_reconstructions.png\")\n",
        "            save_samples(comparison, save_path)\n",
        "            print(f\"Reconstruction examples saved to {save_path}\")\n",
        "    else:\n",
        "        # Load pre-trained autoencoder\n",
        "        try:\n",
        "            autoencoder.load_state_dict(torch.load(ae_path, map_location=device))\n",
        "            print(f\"Loaded autoencoder from {ae_path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Autoencoder checkpoint not found at {ae_path}. Train it first with train_autoencoder=True.\")\n",
        "            return None\n",
        "\n",
        "    # Create latent dataset\n",
        "    if train_model:\n",
        "        print(\"Creating latent dataset...\")\n",
        "        data_loader = DataLoader(mnist_train, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(data_loader, desc=\"Encoding dataset to latents\"):\n",
        "                x = x.to(device)\n",
        "                latents = autoencoder.encode(x)\n",
        "                all_latents.append(latents.cpu())\n",
        "                all_labels.append(y)\n",
        "\n",
        "        latent_data = torch.cat(all_latents, dim=0)\n",
        "        label_data = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Create dataset\n",
        "        latent_dataset = TensorDataset(latent_data, label_data)\n",
        "\n",
        "        # Get latent dimensions for model creation\n",
        "        sample_latent_shape = all_latents[0].shape[1:]\n",
        "        latent_channels = sample_latent_shape[0]\n",
        "\n",
        "        # Create and train latent diffusion model\n",
        "        latent_model = LatentDiffusionModel(\n",
        "            autoencoder=autoencoder,\n",
        "            noise_distribution_fn=noise_distribution_fn,\n",
        "            latent_channels=latent_channels,\n",
        "            hidden_channels=[64, 128, 256],\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"Training latent diffusion model...\")\n",
        "        # We train just the diffusion part, not the autoencoder\n",
        "        # We're using the same training function but passing the diffusion model directly\n",
        "        latent_diffusion, history = train_diffusion_model(\n",
        "            latent_model.diffusion_model,\n",
        "            latent_dataset,\n",
        "            noise_distribution_fn,\n",
        "            num_epochs=50,\n",
        "            batch_size=256,\n",
        "            learning_rate=1e-4,\n",
        "            conditional=True,\n",
        "            checkpoint_path=latent_model_path,\n",
        "            resume_training=False\n",
        "        )\n",
        "\n",
        "        # Update the diffusion model part of latent_model\n",
        "        latent_model.diffusion_model = latent_diffusion\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([h['epoch'] for h in history], [h['loss'] for h in history])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss (Latent Diffusion)')\n",
        "        plt.savefig(os.path.join(CHECKPOINT_DIR, \"latent_model_loss.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    try:\n",
        "        # Load model from checkpoint\n",
        "        print(f\"Loading diffusion model from {latent_model_path}\")\n",
        "        # First get the latent dimensions\n",
        "        with torch.no_grad():\n",
        "            sample_x = torch.zeros(1, 1, 28, 28, device=device)\n",
        "            sample_latent = autoencoder.encode(sample_x)\n",
        "            latent_channels = sample_latent.shape[1]\n",
        "            latent_shape = (latent_channels, sample_latent.shape[2], sample_latent.shape[3])\n",
        "\n",
        "        # Create a latent diffusion model\n",
        "        latent_model = LatentDiffusionModel(\n",
        "            autoencoder=autoencoder,\n",
        "            noise_distribution_fn=noise_distribution_fn,\n",
        "            latent_channels=latent_channels,\n",
        "            hidden_channels=[64, 128, 256],\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        ).to(device)\n",
        "\n",
        "        # Load just the diffusion part\n",
        "        checkpoint = torch.load(latent_model_path, map_location=device)\n",
        "        latent_model.diffusion_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # Generate samples for each digit\n",
        "        for digit in range(10):\n",
        "            print(f\"Generating samples for digit {digit} using latent diffusion...\")\n",
        "\n",
        "            # Generate using the helper method that handles the entire pipeline\n",
        "            samples = latent_model.generate(\n",
        "                num_samples=25,\n",
        "                class_labels=torch.ones(25, dtype=torch.long, device=device) * digit,\n",
        "                num_steps=200,\n",
        "                latent_shape=latent_shape\n",
        "            )\n",
        "\n",
        "            # Save samples\n",
        "            save_path = os.path.join(CHECKPOINT_DIR, f\"latent_model_samples_digit{digit}.png\")\n",
        "            save_samples(samples, save_path)\n",
        "            print(f\"Samples for digit {digit} saved to {save_path}\")\n",
        "\n",
        "        return latent_model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Latent diffusion model checkpoint not found at {latent_model_path}. Train it first with train_model=True.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "6YlabbX-ybaW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 7: Run and Experiment\n",
        "\n",
        "After implementing all the required components, you can run your models and experiment with different\n",
        "parameters to observe their effects on the generated images."
      ],
      "metadata": {
        "id": "BfqIh_7XydzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Choose which models to train and evaluate\n",
        "    # Set parameters to True to train the models, or False to load pre-trained models\n",
        "\n",
        "    # TODO: Uncomment the models you want to train/evaluate\n",
        "\n",
        "    # 1. Basic diffusion model (no conditioning)\n",
        "    # train_and_evaluate_basic_model(train_model=True)\n",
        "\n",
        "    # 2. Conditional diffusion model with attention\n",
        "    # train_and_evaluate_conditional_model(train_model=True)\n",
        "\n",
        "    # 3. Latent diffusion model\n",
        "    # train_and_evaluate_latent_model(train_autoencoder=True, train_model=True)\n",
        "\n",
        "    # Or just evaluate pre-trained models:\n",
        "    # train_and_evaluate_basic_model(train_model=False)\n",
        "    # train_and_evaluate_conditional_model(train_model=False)\n",
        "    # train_and_evaluate_latent_model(train_model=False, train_autoencoder=False)\n",
        "\n",
        "    print(\"\\nDiffusion Model Lab completed! Check the 'model_checkpoints' directory for results.\")\n",
        "\n",
        "    \"\"\"\n",
        "    ## Lab Extensions (Optional)\n",
        "\n",
        "    If you've completed all the tasks and want to explore further, you can try these extensions:\n",
        "\n",
        "    1. Compare the quality and training times of the different approaches\n",
        "    2. Experiment with different noise schedules and model architectures\n",
        "    3. Apply the model to a different dataset (e.g., FashionMNIST)\n",
        "    4. Implement classifier-free guidance for improved conditional generation\n",
        "    5. Add more control to the generation process (e.g., controlling digit size or style)\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "dQ_gtlsix47A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}