{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Diffusion Model Lab: Building Stable Diffusion from Scratch\n",
        "\n",
        "(adapted from Harvard seminars)\n",
        "\n",
        "## Introduction\n",
        "Welcome to the Diffusion Models Laboratory! In this lab, you will implement essential components\n",
        "of a Stable Diffusion-like model and have a working text-to-image generation system by the end.\n",
        "\n",
        "## Learning Objectives\n",
        "By completing this lab, you will:\n",
        "1. Understand the theory behind diffusion models\n",
        "2. Implement key components of a diffusion-based generative model\n",
        "3. Learn how to train and sample from diffusion models\n",
        "4. Experience how attention mechanisms enable text-conditioning\n",
        "5. Explore the efficiency benefits of latent diffusion models\n",
        "\n",
        "## Lab Overview\n",
        "This lab will guide you through building a complete diffusion-based generative model with the following components:\n",
        "1) Forward and reverse diffusion processes for generating new content\n",
        "2) Neural networks with appropriate inductive biases for images (U-Net architecture)\n",
        "3) Cross-attention mechanisms to bridge text and visual features\n",
        "4) Latent spaces for efficient image representation via autoencoders\n",
        "\n",
        "We'll use the MNIST dataset (28x28 pixel handwritten digits) as our training data to make the\n",
        "process reasonably fast. By the end, your model will convert a text prompt like \"7\" into\n",
        "an image of the handwritten digit 7.\n",
        "\n",
        "## Lab Structure\n",
        "This lab is divided into several tasks, each focusing on a different component of diffusion models:\n",
        "- Task 1: Implement forward and reverse diffusion processes\n",
        "- Task 2: Implement the U-Net architecture for diffusion\n",
        "- Task 3: Develop loss functions and sampling mechanisms\n",
        "- Task 4: Build attention mechanisms for conditioning\n",
        "- Task 5: Create a latent diffusion model with an autoencoder\n",
        "\n",
        "For each task, you'll find:\n",
        "- Theoretical background explaining key concepts\n",
        "- Starter code with TODOs for you to complete\n",
        "- Testing functions to verify your implementations\n",
        "- Visualization helpers to understand what's happening\n",
        "\n",
        "## Managing computational resources\n",
        "If you encounter resource limitations (e.g., end of Colab quota or VPN issues):\n",
        "- Use CPU for training (can be done on Colab or your laptop)\n",
        "- Implement checkpointing to save model progress (save checkpoints to Google Drive in Colab)\n",
        "- Load from checkpoints to continue training from where you left off\n",
        "\n",
        "### Google Drive Integration\n",
        "When running this lab in Google Colab, you can save your model checkpoints to Google Drive, which prevents\n",
        "losing your training progress if your Colab session crashes or times out. To enable this:\n",
        "\n",
        "1. Set `USE_GOOGLE_DRIVE = True` near the top of the code\n",
        "2. When prompted, authorize the connection to your Google Drive\n",
        "3. Your checkpoints will be saved to the \"diffusion_model_checkpoints\" folder in your Drive\n",
        "\n",
        "You can always resume training from these checkpoints even if you start a new Colab session.\n",
        "\n",
        "Let's get started with building diffusion models!"
      ],
      "metadata": {
        "id": "SAkqLz21x6kP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Required library imports\n",
        "import functools\n",
        "import os  # Added for file operations\n",
        "import time  # Added for timing operations\n",
        "import math\n",
        "from pathlib import Path  # Added for path handling\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torchvision.datasets import MNIST\n",
        "from torchvision.utils import make_grid\n",
        "from tqdm import tqdm, trange\n",
        "from einops import rearrange"
      ],
      "metadata": {
        "id": "viwKukETyFF0"
      },
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Detect if running in Google Colab\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running in Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Not running in Google Colab\")\n",
        "\n",
        "# Function to setup Google Drive for checkpoint storage\n",
        "def setup_checkpoint_dir(use_google_drive=False):\n",
        "    \"\"\"\n",
        "    Sets up the directory for storing model checkpoints.\n",
        "\n",
        "    Args:\n",
        "        use_google_drive: If True and running in Colab, mounts Google Drive\n",
        "                         and creates checkpoint directory there\n",
        "\n",
        "    Returns:\n",
        "        Path to the checkpoint directory\n",
        "    \"\"\"\n",
        "    if use_google_drive and IN_COLAB:\n",
        "        try:\n",
        "            from google.colab import drive\n",
        "            # Mount Google Drive\n",
        "            drive.mount('/content/drive')\n",
        "\n",
        "            # Create checkpoint directory in Google Drive\n",
        "            checkpoint_dir = \"/content/drive/MyDrive/diffusion_model_checkpoints\"\n",
        "            os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "            print(f\"Checkpoint directory set to Google Drive: {checkpoint_dir}\")\n",
        "            return checkpoint_dir\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to set up Google Drive for checkpoints: {e}\")\n",
        "            print(\"Falling back to local checkpoint directory\")\n",
        "\n",
        "    # Use local directory if not using Google Drive or if setup failed\n",
        "    checkpoint_dir = \"model_checkpoints\"\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(f\"Checkpoint directory set to local path: {checkpoint_dir}\")\n",
        "    return checkpoint_dir"
      ],
      "metadata": {
        "id": "SBNzlojnyHjU",
        "outputId": "323ba750-4659-4cc5-9c97-6a6be0310386",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Google Colab\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up device configuration\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "uaa5VFnuyORs",
        "outputId": "224cba63-1062-40f7-ed62-8f2f1d916e69",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: Use Google Drive for checkpoints when running in Colab\n",
        "# Set to True to save checkpoints to Google Drive\n",
        "USE_GOOGLE_DRIVE = False  # Change to True to use Google Drive for checkpoints\n",
        "\n",
        "# Create directory for saving models\n",
        "CHECKPOINT_DIR = setup_checkpoint_dir(use_google_drive=USE_GOOGLE_DRIVE)"
      ],
      "metadata": {
        "id": "A1exTfMsySWu",
        "outputId": "751070ae-3ef6-4a6f-f278-e21789523b47",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 218,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checkpoint directory set to local path: model_checkpoints\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Forward and Reverse Diffusion Processes\n",
        "\n",
        "The core concept of diffusion models is to gradually add noise to data samples until they become\n",
        "pure noise, then learn to reverse this process. This allows us to generate new samples by starting\n",
        "with random noise and applying the learned denoising process.\n",
        "\n",
        "### Forward Diffusion\n",
        "In forward diffusion, we gradually add noise to our data samples according to:\n",
        "\n",
        "x(t + Δt) = x(t) + σ(t) √(Δt) * ε\n",
        "\n",
        "where:\n",
        "- σ(t) is the noise strength at time t\n",
        "- Δt is the step size\n",
        "- ε ~ N(0, 1) is a standard normal random variable\n",
        "\n",
        "As t increases, our sample becomes increasingly noisy until it's indistinguishable from pure noise.\n",
        "\n",
        "### Task 1.1: Implement the Forward Diffusion Process"
      ],
      "metadata": {
        "id": "SwPOoLOEyWSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example noise strength function: constant value regardless of time\n",
        "def constant_noise_strength(t):\n",
        "    \"\"\"Returns a constant noise strength regardless of time.\"\"\"\n",
        "    return 1.0\n",
        "\n",
        "# TODO: Implement the forward diffusion process\n",
        "def forward_diffusion_1D(initial_sample, noise_strength_fn, initial_time, num_steps, step_size):\n",
        "    \"\"\"\n",
        "    Simulates the forward diffusion process in 1D.\n",
        "\n",
        "    Args:\n",
        "        initial_sample: Starting value (scalar)\n",
        "        noise_strength_fn: Function that returns noise strength at given time\n",
        "        initial_time: Starting time value\n",
        "        num_steps: Number of diffusion steps to simulate\n",
        "        step_size: Size of each time step\n",
        "\n",
        "    Returns:\n",
        "        tuple: (trajectory array, time array)\n",
        "    \"\"\"\n",
        "    # Initialize arrays to store the trajectory and time values\n",
        "    trajectory = np.zeros(num_steps + 1); trajectory[0] = initial_sample\n",
        "    time_values = initial_time + np.arange(num_steps + 1)*step_size\n",
        "\n",
        "    # Perform Euler-Maruyama steps for the diffusion process\n",
        "    for i in range(num_steps):\n",
        "        # 1. Get the current noise strength using noise_strength_fn\n",
        "        noise_strength = noise_strength_fn(time_values[i])\n",
        "\n",
        "        # 2. Sample random noise from a standard normal distribution\n",
        "        noise = np.random.normal()\n",
        "        # 3. Update the trajectory according to the forward diffusion equation\n",
        "        trajectory[i+1] = trajectory[i] + noise_strength * np.sqrt(step_size) * noise\n",
        "\n",
        "\n",
        "    return trajectory, time_values\n",
        "\n",
        "\n",
        "# Test the forward diffusion function\n",
        "def visualize_forward_diffusion():\n",
        "    \"\"\"Visualize multiple forward diffusion trajectories starting from the same point.\"\"\"\n",
        "    num_steps = 100\n",
        "    initial_time = 0\n",
        "    step_size = 0.1\n",
        "    initial_sample = 0\n",
        "    num_trajectories = 5\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(num_trajectories):\n",
        "        trajectory, time_values = forward_diffusion_1D(\n",
        "            initial_sample,\n",
        "            constant_noise_strength,\n",
        "            initial_time,\n",
        "            num_steps,\n",
        "            step_size\n",
        "        )\n",
        "        plt.plot(time_values, trajectory)\n",
        "\n",
        "    plt.xlabel('Time', fontsize=14)\n",
        "    plt.ylabel('Sample Value', fontsize=14)\n",
        "    plt.title('Forward Diffusion Process - Multiple Trajectories', fontsize=16)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "921TCcg9yeEy"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reverse Diffusion\n",
        "\n",
        "The reverse diffusion process allows us to generate new data by starting from noise and\n",
        "progressively denoising it. The reverse process is given by:\n",
        "\n",
        "x(t + Δt) = x(t) + σ(T-t)² · ∇_x log p(x, T-t) · Δt + σ(T-t) · √(Δt) · ε\n",
        "\n",
        "The function ∇_x log p(x, t) is called the \"score function\" and represents the gradient of the\n",
        "log probability density. If we can learn this score function, we can reverse the diffusion process\n",
        "to generate data from noise.\n",
        "\n",
        "For the special case where our initial distribution is concentrated at x₀ = 0 and the noise strength\n",
        "is constant, the score function has the closed form:\n",
        "\n",
        "s(x, t) = -x/(σ² · t)"
      ],
      "metadata": {
        "id": "Q4OWnuZFygWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Score function for the special case\n",
        "def simple_score_function(x, x0, noise_strength, t):\n",
        "    \"\"\"\n",
        "    Score function for the special case where the initial distribution\n",
        "    is concentrated at x0 and noise strength is constant.\n",
        "\n",
        "    Args:\n",
        "        x: Current sample value\n",
        "        x0: Initial distribution center (typically 0)\n",
        "        noise_strength: Current noise strength\n",
        "        t: Current time\n",
        "\n",
        "    Returns:\n",
        "        Score value\n",
        "    \"\"\"\n",
        "    score = score = - (x-x0)/((noise_strength**2)*t)\n",
        "\n",
        "    return score"
      ],
      "metadata": {
        "id": "7y2B_yJwymMA"
      },
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 1.2: Implement the Reverse Diffusion Process"
      ],
      "metadata": {
        "id": "j-jIOreCyoqp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the reverse diffusion process\n",
        "def reverse_diffusion_1D(initial_sample, noise_strength_fn, score_fn, final_time, num_steps, step_size):\n",
        "    \"\"\"\n",
        "    Simulates the reverse diffusion process in 1D.\n",
        "\n",
        "    Args:\n",
        "        initial_sample: Starting value (scalar, typically sampled from noise distribution)\n",
        "        noise_strength_fn: Function that returns noise strength at given time\n",
        "        score_fn: Score function that guides the reverse process\n",
        "        final_time: Total diffusion time (T)\n",
        "        num_steps: Number of reverse steps to simulate\n",
        "        step_size: Size of each time step\n",
        "\n",
        "    Returns:\n",
        "        tuple: (trajectory array, time array)\n",
        "    \"\"\"\n",
        "    # Initialize arrays to store the trajectory and time values\n",
        "    trajectory = np.zeros(num_steps + 1); trajectory[0] = initial_sample\n",
        "    time_values = np.arange(num_steps + 1)*step_size\n",
        "\n",
        "    for i in range(num_steps):\n",
        "      #   1. Calculate the reverse time (final_time - current_time)\n",
        "      reverse_time = final_time - time_values[i]\n",
        "      #   2. Get the current noise strength\n",
        "      noise_strength = noise_strength_fn(reverse_time)\n",
        "      #   3. Calculate the score value using the score function\n",
        "      score = score_fn(time_values[i], 0, noise_strength, final_time-time_values[i])\n",
        "\n",
        "      # Perform reverse diffusion steps\n",
        "      # 4. Generate random noise\n",
        "      noise = np.random.normal()\n",
        "      # 5. Update the trajectory according to the reverse diffusion equation\n",
        "      trajectory[i+1] = trajectory[i] + (noise_strength**2) * score * step_size + noise_strength * np.sqrt(step_size) * noise\n",
        "\n",
        "    return trajectory, time_values\n",
        "\n",
        "# Test the reverse diffusion function\n",
        "def visualize_reverse_diffusion():\n",
        "    \"\"\"Visualize multiple reverse diffusion trajectories starting from noise.\"\"\"\n",
        "    num_steps = 100\n",
        "    step_size = 0.1\n",
        "    final_time = 11\n",
        "    num_trajectories = 5\n",
        "\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    for i in range(num_trajectories):\n",
        "        # Start from a noisy point (sample from the noise distribution)\n",
        "        noisy_sample = np.random.normal(loc=0, scale=final_time)\n",
        "\n",
        "        trajectory, time_values = reverse_diffusion_1D(\n",
        "            noisy_sample,\n",
        "            constant_noise_strength,\n",
        "            simple_score_function,\n",
        "            final_time,\n",
        "            num_steps,\n",
        "            step_size\n",
        "        )\n",
        "        plt.plot(time_values, trajectory)\n",
        "\n",
        "    plt.xlabel('Time', fontsize=14)\n",
        "    plt.ylabel('Sample Value', fontsize=14)\n",
        "    plt.title('Reverse Diffusion Process - Multiple Trajectories', fontsize=16)\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "gz0aCUHhywm6"
      },
      "execution_count": 221,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Neural Network Architecture for Image Diffusion\n",
        "\n",
        "To effectively apply diffusion models to images, we need neural network architectures that work well\n",
        "with spatial data. The U-Net architecture is an excellent choice because it:\n",
        "\n",
        "1. Maintains spatial information through skip connections\n",
        "2. Processes images at multiple resolutions via downsampling and upsampling\n",
        "3. Can effectively capture both local and global image features\n",
        "\n",
        "Since our score function must be time-dependent, we also need a way to condition our network on time.\n",
        "We'll use sinusoidal time embeddings that convert a scalar time value into a high-dimensional vector.\n",
        "\n",
        "### Task 2.1: Implement Time Embedding Modules"
      ],
      "metadata": {
        "id": "Xi-vBgzly0IJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Time embedding modules\n",
        "class GaussianFourierProjection(nn.Module):\n",
        "    \"\"\"\n",
        "    Gaussian random features for encoding time steps using sinusoidal functions.\n",
        "    This creates a high-dimensional representation of time that helps the model\n",
        "    distinguish between different diffusion timesteps.\n",
        "    \"\"\"\n",
        "    def __init__(self, embedding_dim, scale=30.0):\n",
        "        super().__init__()\n",
        "        # Randomly sample frequencies during initialization\n",
        "        # These frequencies are fixed during optimization\n",
        "        self.weight = nn.Parameter(torch.randn(embedding_dim // 2) * scale, requires_grad=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Time values [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            Time embeddings [batch_size, embedding_dim]\n",
        "        \"\"\"\n",
        "\n",
        "        x_proj = x[:, None] * self.weight[None, :] * 2 * np.pi\n",
        "        return torch.cat([torch.sin(x_proj), torch.cos(x_proj)], dim=-1)\n",
        "\n",
        "\n",
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    A dense layer that reshapes time embeddings for injection into convolutional layers.\n",
        "    This allows time information to be added directly to feature maps.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.dense = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: Time embeddings [batch_size, input_dim]\n",
        "\n",
        "        Returns:\n",
        "            Reshaped time embeddings [batch_size, output_dim, 1, 1]\n",
        "            which can be broadcast-added to convolutional feature maps\n",
        "        \"\"\"\n",
        "        x = self.dense(x)  # batch_size, output_dim\n",
        "        x = x.unsqueeze(-1).unsqueeze(-1)\n",
        "        return x"
      ],
      "metadata": {
        "id": "a4Uz7L0By537"
      },
      "execution_count": 222,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.2: Implement the U-Net Architecture for Diffusion Models"
      ],
      "metadata": {
        "id": "2iOT16ZMzBKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionUNet(nn.Module):\n",
        "    \"\"\"\n",
        "    U-Net architecture for image-based diffusion models with time conditioning.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_distribution_fn, channels=[32, 64, 128, 256], embed_dim=256):\n",
        "        \"\"\"\n",
        "        Initialize a time-dependent score-based network based on U-Net architecture.\n",
        "\n",
        "        Args:\n",
        "            noise_distribution_fn: Function that maps time t to the standard deviation\n",
        "                                  of the perturbation kernel p(x(t)|x(0))\n",
        "            channels: Number of channels for feature maps at each resolution\n",
        "            embed_dim: Dimensionality of time embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Time embedding layers\n",
        "        self.time_embed = nn.Sequential(\n",
        "            GaussianFourierProjection(embedding_dim=embed_dim),\n",
        "            nn.Linear(embed_dim, embed_dim)\n",
        "        )\n",
        "\n",
        "        self.conv_in = nn.Conv2d(1, channels[0], kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # Create a TimeEmbedding layer to inject time information\n",
        "        self.time_embed1 = TimeEmbedding(embed_dim, channels[0])\n",
        "        self.norm1 = nn.GroupNorm(4, num_channels=channels[0])\n",
        "\n",
        "        self.conv2 = nn.Conv2d(channels[0], channels[1], kernel_size=3, stride=2, bias=False)\n",
        "        self.time_embed2 = TimeEmbedding(embed_dim, channels[1])\n",
        "        self.gnorm2 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "\n",
        "        self.conv3 = nn.Conv2d(channels[1], channels[2], 3, stride=2, bias=False)\n",
        "        self.dense3 = TimeEmbedding(embed_dim, channels[2])\n",
        "        self.gnorm3 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "\n",
        "        self.conv4 = nn.Conv2d(channels[2], channels[3], 3, stride=2, bias=False)\n",
        "        self.dense4 = TimeEmbedding(embed_dim, channels[3])\n",
        "        self.gnorm4 = nn.GroupNorm(32, num_channels=channels[3])\n",
        "\n",
        "        # TODO: Define the decoding (upsampling) path with skip connections\n",
        "        # For each level in the upsampling path:\n",
        "        # 1. Create a ConvTranspose2d layer\n",
        "        # 2. Create a TimeEmbedding layer\n",
        "        # 3. Add normalization\n",
        "\n",
        "        # YOUR CODE HERE - Decoding Path\n",
        "\n",
        "        self.upconv4 = nn.ConvTranspose2d(channels[3], channels[2], 3, stride=2, output_padding=1, bias=False)\n",
        "        self.time_embed_up4 = TimeEmbedding(embed_dim, channels[2])\n",
        "        self.upnorm4 = nn.GroupNorm(32, num_channels=channels[2])\n",
        "\n",
        "        self.upconv3 = nn.ConvTranspose2d(channels[2]*2, channels[1], 3, stride=2, output_padding=1, bias=False)\n",
        "        self.time_embed_up3 = TimeEmbedding(embed_dim, channels[1])\n",
        "        self.upnorm3 = nn.GroupNorm(32, num_channels=channels[1])\n",
        "\n",
        "        self.upconv2 = nn.ConvTranspose2d(channels[1]*2, channels[0], 3, stride=2, output_padding=1, bias=False)\n",
        "        self.time_embed_up2 = TimeEmbedding(embed_dim, channels[0])\n",
        "        self.upnorm2 = nn.GroupNorm(32, num_channels=channels[0])\n",
        "\n",
        "        # output layer\n",
        "        self.conv_out = nn.Conv2d(channels[0]*2, 1, kernel_size=3, stride=1, padding=1)\n",
        "\n",
        "        # Activation function: Swish/SiLU\n",
        "        self.activation = nn.SiLU()\n",
        "\n",
        "        # Store noise function for normalization during inference\n",
        "        self.noise_distribution_fn = noise_distribution_fn\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the U-Net.\n",
        "\n",
        "        Args:\n",
        "            x: Input images [batch_size, 1, height, width]\n",
        "            t: Diffusion timesteps [batch_size]\n",
        "            class_labels: Optional class conditioning (not used in base model)\n",
        "\n",
        "        Returns:\n",
        "            Predicted score (noise estimate) [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass through the U-Net\n",
        "        # 1. Get time embeddings\n",
        "        # 2. Pass through the encoding path\n",
        "        # 3. Pass through the decoding path with skip connections\n",
        "        # 4. Apply the output layer\n",
        "        # 5. Normalize the output by the noise level\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        # Obtain the Gaussian random feature embedding for t\n",
        "        temb = self.activation(self.time_embed(t))\n",
        "\n",
        "        h1 = self.conv_in(x)\n",
        "        h1 += self.time_embed1(temb)\n",
        "        h1 = self.norm1(h1)\n",
        "        h1 = self.activation(h1)\n",
        "\n",
        "        h2 = self.conv2(h1)\n",
        "        h2 += self.time_embed2(temb)\n",
        "        h2 = self.gnorm2(h2)\n",
        "        h2 = self.activation(h2)\n",
        "\n",
        "        h3 = self.conv3(h2)\n",
        "        h3 += self.dense3(temb)\n",
        "        h3 = self.gnorm3(h3)\n",
        "        h3 = self.activation(h3)\n",
        "\n",
        "        h4 = self.conv4(h3)\n",
        "        h4 += self.dense4(temb)\n",
        "        h4 = self.gnorm4(h4)\n",
        "        h4 = self.activation(h4)\n",
        "\n",
        "        h = self.upconv4(h4)\n",
        "        h += self.time_embed_up4(temb)\n",
        "        h = self.upnorm4(h)\n",
        "        h = self.activation(h)\n",
        "        h = torch.cat([h, h3], dim=1)  # skip connection\n",
        "\n",
        "        h = self.upconv3(h)\n",
        "        h += self.time_embed_up3(temb)\n",
        "        h = self.upnorm3(h)\n",
        "        h = self.activation(h)\n",
        "        h = torch.cat([h, h2], dim=1)  # skip connection\n",
        "\n",
        "        h = self.upconv2(h)\n",
        "        h += self.time_embed_up2(temb)\n",
        "        h = self.upnorm2(h)\n",
        "        h = self.activation(h)\n",
        "        h = torch.cat([h, h1], dim=1)  # skip connection\n",
        "        h = self.conv_out(h)\n",
        "\n",
        "        sigma_t = self.noise_distribution_fn(t)[:, None, None, None]\n",
        "        return h / sigma_t"
      ],
      "metadata": {
        "id": "B3TCLJ0NzR4V"
      },
      "execution_count": 223,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 2.3: Implement a Residual U-Net Variant (Optional - for bonus points)\n",
        "\n",
        "For advanced students: implement a variant of the U-Net with residual connections\n",
        "instead of concatenation for skip connections."
      ],
      "metadata": {
        "id": "STi30iQWzUc-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionUNetResidual(nn.Module):\n",
        "    \"\"\"\n",
        "    Alternative U-Net architecture with residual connections instead of concatenation.\n",
        "    \"\"\"\n",
        "    def __init__(self, noise_distribution_fn, channels=[32, 64, 128, 256], embed_dim=256):\n",
        "        \"\"\"\n",
        "        Initialize a U-Net with residual skip connections instead of concatenation.\n",
        "\n",
        "        Args:\n",
        "            noise_distribution_fn: Function that maps time t to the standard deviation\n",
        "            channels: Number of channels for feature maps at each resolution\n",
        "            embed_dim: Dimensionality of time embeddings\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Implement the residual U-Net architecture\n",
        "        # The structure is similar to the regular U-Net but uses addition\n",
        "        # instead of concatenation for skip connections\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass of the U-Net with residual connections.\n",
        "\n",
        "        Args:\n",
        "            x: Input images [batch_size, 1, height, width]\n",
        "            t: Diffusion timesteps [batch_size]\n",
        "            class_labels: Optional class conditioning (not used in this model)\n",
        "\n",
        "        Returns:\n",
        "            Predicted score (noise estimate) [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the forward pass with residual connections\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        return None  # Replace with your implementation"
      ],
      "metadata": {
        "id": "zuTBREfIz5E7"
      },
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Training the Diffusion Model\n",
        "\n",
        "Now we'll define the specific diffusion process and the loss function to train our model.\n",
        "Our forward process follows the stochastic differential equation (SDE):\n",
        "\n",
        "dx = σ^t dw\n",
        "\n",
        "where σ is a noise parameter and w is the Wiener process (Brownian motion).\n",
        "\n",
        "### Task 3.1: Implement the Noise Distribution and Diffusion Functions"
      ],
      "metadata": {
        "id": "5mciyRZs0Bsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the marginal noise distribution\n",
        "def marginal_prob_std(t, sigma=25.0):\n",
        "    \"\"\"\n",
        "    Compute the standard deviation of p_{0t}(x(t) | x(0)).\n",
        "\n",
        "    Args:\n",
        "        t: Time values [batch_size]\n",
        "        sigma: Base noise multiplier\n",
        "\n",
        "    Returns:\n",
        "        Standard deviation at time t\n",
        "    \"\"\"\n",
        "\n",
        "    t = torch.tensor(t, device=device)\n",
        "    return torch.sqrt((sigma**(2 * t) - 1.) / 2. / np.log(sigma))\n",
        "\n",
        "\n",
        "def diffusion_coeff(t, sigma=25.0):\n",
        "    \"\"\"\n",
        "    Compute the diffusion coefficient of the SDE.\n",
        "\n",
        "    Args:\n",
        "        t: Time values [batch_size]\n",
        "        sigma: Base noise multiplier\n",
        "\n",
        "    Returns:\n",
        "        Diffusion coefficient at time t\n",
        "    \"\"\"\n",
        "    return torch.tensor(sigma**t, device=device)\n",
        "\n",
        "# Create functions with default parameter\n",
        "noise_distribution_fn = functools.partial(marginal_prob_std, sigma=25.0)\n",
        "diffusion_coeff_fn = functools.partial(diffusion_coeff, sigma=25.0)"
      ],
      "metadata": {
        "id": "kJRkYFWAoX_B"
      },
      "execution_count": 225,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.2: Implement the Diffusion Loss Function"
      ],
      "metadata": {
        "id": "gCcn8af8ohna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the loss function for training the diffusion model\n",
        "def diffusion_loss_fn(model, images, noise_distribution_fn, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Loss function for training score-based generative models.\n",
        "\n",
        "    Args:\n",
        "        model: The score model network\n",
        "        images: Training images [batch_size, 1, height, width]\n",
        "        noise_distribution_fn: Function computing noise std at time t\n",
        "        epsilon: Small value for numerical stability\n",
        "\n",
        "    Returns:\n",
        "        Average loss value for the batch\n",
        "    \"\"\"\n",
        "    # TODO: Implement the diffusion loss function with the following steps:\n",
        "\n",
        "    # 5. Predict the noise component using the model\n",
        "    # 6. Calculate MSE between predicted and actual noise, properly weighted\n",
        "\n",
        "    # 1. Sample random timesteps for each image\n",
        "    batch_size = images.shape[0]\n",
        "    random_t = torch.rand(batch_size, device=images.device) * (1.0 - epsilon) + epsilon\n",
        "    # 2. Get the noise standard deviation at sampled times\n",
        "    std = marginal_prob_std(random_t)\n",
        "\n",
        "    ##### YOUR CODE HERE\n",
        "    # TODO: 3. Generate random noise with the same shape as input images\n",
        "    z = torch.randn_like(images)\n",
        "    # TODO: 4. Add noise to the input images according to the noise schedule\n",
        "\n",
        "    # Sample random timesteps for each image\n",
        "    # batch_size = images.shape[0]\n",
        "    # random_t = torch.rand(batch_size, device=images.device) * (1. - epsilon) + epsilon\n",
        "\n",
        "    # Get the noise standard deviation\n",
        "    noise_std = noise_distribution_fn(random_t)  # [batch_size]\n",
        "\n",
        "    # Generate random noise\n",
        "    noise = torch.randn_like(images)\n",
        "\n",
        "    # Add noise to the images\n",
        "    noisy_images = images + noise_std[:, None, None, None] * z\n",
        "\n",
        "    # Predict the noise component\n",
        "    predicted_noise = model(noisy_images, random_t)\n",
        "\n",
        "    # Calculate weighted MSE loss\n",
        "    loss = torch.mean(torch.sum((score * std[:, None, None, None] + z)**2, dim=(1,2,3)))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "QpT_NOdjpgr_"
      },
      "execution_count": 226,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.3: Implement the Conditional Loss Function"
      ],
      "metadata": {
        "id": "qxuR3d16poo2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the conditional loss function for class-conditioned generation\n",
        "def conditional_diffusion_loss_fn(model, images, class_labels, noise_distribution_fn, epsilon=1e-5):\n",
        "    \"\"\"\n",
        "    Loss function for training class-conditioned diffusion models.\n",
        "\n",
        "    Args:\n",
        "        model: The score model network\n",
        "        images: Training images [batch_size, 1, height, width]\n",
        "        class_labels: Class labels for conditioning [batch_size]\n",
        "        noise_distribution_fn: Function computing noise std at time t\n",
        "        epsilon: Small value for numerical stability\n",
        "\n",
        "    Returns:\n",
        "        Average loss value for the batch\n",
        "    \"\"\"\n",
        "    batch_size = images.shape[0]\n",
        "    random_t = torch.rand(batch_size, device=images.device) * (1.0 - epsilon) + epsilon\n",
        "    noise_std = noise_distribution_fn(random_t)\n",
        "    noise = torch.randn_like(images)\n",
        "\n",
        "    noisy_images = images + noise_std[:, None, None, None] * noise\n",
        "\n",
        "    predicted_noise = model(noisy_images, random_t, class_labels=class_labels)\n",
        "    loss = torch.mean(torch.sum((predicted_noise - noise)**2, dim=(1, 2, 3)))\n",
        "\n",
        "    return loss"
      ],
      "metadata": {
        "id": "2VR_UvZdtp9d"
      },
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 3.4: Implement the Euler-Maruyama Sampler"
      ],
      "metadata": {
        "id": "e1Z8I_Astr34"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def euler_maruyama_sampler(\n",
        "    score_model,\n",
        "    noise_distribution_fn,\n",
        "    diffusion_coeff_fn,\n",
        "    batch_size=64,\n",
        "    img_shape=(1, 28, 28),\n",
        "    num_steps=500,\n",
        "    device=\"cuda\",\n",
        "    epsilon=1e-3,\n",
        "    class_labels=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate samples from the score-based model using Euler-Maruyama solver.\n",
        "\n",
        "    Args:\n",
        "        score_model: The trained score model\n",
        "        noise_distribution_fn: Function mapping time to noise std\n",
        "        diffusion_coeff_fn: Function mapping time to diffusion coefficient\n",
        "        batch_size: Number of samples to generate\n",
        "        img_shape: Shape of a single image\n",
        "        num_steps: Number of sampling steps\n",
        "        device: Device to run on ('cuda' or 'cpu')\n",
        "        epsilon: Small time value for numerical stability\n",
        "        class_labels: Optional class labels for conditional generation\n",
        "\n",
        "    Returns:\n",
        "        Generated samples [batch_size, channels, height, width]\n",
        "    \"\"\"\n",
        "\n",
        "    # Start from pure noise\n",
        "    t = torch.ones(batch_size, device=device)\n",
        "    init_noise = torch.randn(batch_size, *img_shape, device=device) * marginal_prob_std(t)[:, None, None, None]\n",
        "\n",
        "    # Setup time steps\n",
        "    time_steps = torch.linspace(1., epsilon, num_steps, device=device)\n",
        "    step_size = time_steps[0] - time_steps[1]\n",
        "\n",
        "    # Progressive denoising\n",
        "    x = init_noise\n",
        "    with torch.no_grad():\n",
        "        for time_step in tqdm(time_steps):\n",
        "          batch_time_step = torch.ones(batch_size, device=device) * time_step\n",
        "          g = diffusion_coeff(batch_time_step)\n",
        "          mean_x = x + (g**2)[:, None, None, None] * score_model(x, batch_time_step, y=class_labels) * step_size\n",
        "          x = mean_x + torch.sqrt(step_size) * g[:, None, None, None] * torch.randn_like(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "# Utility function to save generated samples\n",
        "def save_samples(samples, filename):\n",
        "    \"\"\"\n",
        "    Save generated samples as an image.\n",
        "\n",
        "    Args:\n",
        "        samples: Generated samples [batch_size, channels, height, width]\n",
        "        filename: Path to save the image\n",
        "    \"\"\"\n",
        "    # Ensure pixel values are in valid range\n",
        "    samples = samples.clamp(0.0, 1.0)\n",
        "\n",
        "    # Create a grid of images\n",
        "    sample_grid = make_grid(samples, nrow=int(np.sqrt(samples.shape[0])))\n",
        "\n",
        "    # Convert to numpy and transpose dimensions for matplotlib\n",
        "    sample_np = sample_grid.permute(1, 2, 0).cpu().numpy()\n",
        "\n",
        "    # Save using matplotlib\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.axis('off')\n",
        "    plt.imshow(sample_np, vmin=0., vmax=1.)\n",
        "    plt.savefig(filename, bbox_inches='tight', pad_inches=0.1, dpi=300)\n",
        "    plt.close()\n",
        "\n",
        "    return sample_np  # Return for display"
      ],
      "metadata": {
        "id": "OXRVAmNuvbOt"
      },
      "execution_count": 228,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Training Utility Functions\n",
        "\n",
        "Below are utility functions for training diffusion models and saving/loading checkpoints.\n",
        "These functions include proper checkpointing to allow resuming training sessions.\n",
        "\n",
        "### Task 4.1: Implement Model Training Function with Checkpointing"
      ],
      "metadata": {
        "id": "mb_nysoCvfbF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the diffusion model training function\n",
        "def train_diffusion_model(\n",
        "    model,\n",
        "    dataset,\n",
        "    noise_distribution_fn,\n",
        "    num_epochs=50,\n",
        "    batch_size=256,\n",
        "    learning_rate=1e-4,\n",
        "    conditional=False,\n",
        "    checkpoint_freq=5,\n",
        "    checkpoint_path=None,  # Changed from a default string to None\n",
        "    resume_training=False\n",
        "):\n",
        "    \"\"\"\n",
        "    Train a diffusion model with checkpointing.\n",
        "\n",
        "    Args:\n",
        "        model: The diffusion model to train\n",
        "        dataset: Dataset for training\n",
        "        noise_distribution_fn: Function to compute noise std at different times\n",
        "        num_epochs: Number of epochs to train for\n",
        "        batch_size: Batch size for training\n",
        "        learning_rate: Learning rate for the optimizer\n",
        "        conditional: Whether the model is class-conditional\n",
        "        checkpoint_freq: How often to save checkpoints (in epochs)\n",
        "        checkpoint_path: Path to save/load model checkpoints. If None, uses CHECKPOINT_DIR.\n",
        "        resume_training: Whether to resume from a checkpoint\n",
        "\n",
        "    Returns:\n",
        "        Trained model and training history\n",
        "    \"\"\"\n",
        "       # Set default checkpoint path if not provided\n",
        "    if checkpoint_path is None:\n",
        "        checkpoint_filename = \"conditional_diffusion.pt\" if conditional else \"diffusion_model.pt\"\n",
        "        checkpoint_path = os.path.join(CHECKPOINT_DIR, checkpoint_filename)\n",
        "\n",
        "    # Setup data loader\n",
        "    data_loader = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        pin_memory=True,\n",
        "        num_workers=4\n",
        "    )\n",
        "\n",
        "    # Initialize optimizer and scheduler\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Initialize tracking variables\n",
        "    start_epoch = 0\n",
        "    training_history = []\n",
        "\n",
        "    # Resume from checkpoint if requested\n",
        "    if resume_training and os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        training_history = checkpoint['training_history']\n",
        "        print(f\"Resuming training from epoch {start_epoch}\")\n",
        "\n",
        "    # Create full path for checkpoint\n",
        "    os.makedirs(os.path.dirname(os.path.abspath(checkpoint_path)), exist_ok=True)\n",
        "\n",
        "    # Setup training loop\n",
        "    model.train()\n",
        "    device = next(model.parameters()).device\n",
        "    progress_bar = trange(start_epoch, num_epochs, desc=\"Training\")\n",
        "\n",
        "    for epoch in progress_bar:\n",
        "        epoch_loss = 0.0\n",
        "        num_batches = 0\n",
        "\n",
        "        for batch in data_loader:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if conditional:\n",
        "                images, class_labels = batch\n",
        "                images = images.to(device)\n",
        "                class_labels = class_labels.to(device)\n",
        "                loss = conditional_diffusion_loss_fn(\n",
        "                    model, images, class_labels, noise_distribution_fn\n",
        "                )\n",
        "            else:\n",
        "                images = batch[0].to(device) if isinstance(batch, list) else batch.to(device)\n",
        "                loss = diffusion_loss_fn(model, images, noise_distribution_fn)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        avg_loss = epoch_loss / num_batches\n",
        "        training_history.append({\n",
        "            'epoch': epoch + 1,\n",
        "            'loss': epoch_loss / num_batches\n",
        "        })\n",
        "\n",
        "        progress_bar.set_postfix({\n",
        "            'epoch': epoch + 1,\n",
        "            'loss': avg_loss,\n",
        "            'lr': scheduler.get_last_lr()[0]\n",
        "        })\n",
        "\n",
        "        # Save checkpoint periodically\n",
        "        if (epoch + 1) % checkpoint_freq == 0 or (epoch + 1) == num_epochs:\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler_state_dict': scheduler.state_dict(),\n",
        "                'training_history': training_history,\n",
        "                'loss': avg_loss,\n",
        "            }, checkpoint_path)\n",
        "\n",
        "    # Final model saving\n",
        "    model.eval()\n",
        "    return model, training_history"
      ],
      "metadata": {
        "id": "uYqer636vwbE"
      },
      "execution_count": 255,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 4.2: Implement Model Loading Function"
      ],
      "metadata": {
        "id": "16DIMMKbvxMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the function to load a model from a checkpoint\n",
        "def load_model_from_checkpoint(\n",
        "    model_class,\n",
        "    checkpoint_path,\n",
        "    noise_distribution_fn,\n",
        "    map_location=None,\n",
        "    **model_kwargs\n",
        "):\n",
        "    \"\"\"\n",
        "    Load a diffusion model from a checkpoint.\n",
        "\n",
        "    Args:\n",
        "        model_class: The model class to instantiate\n",
        "        checkpoint_path: Path to the checkpoint file\n",
        "        noise_distribution_fn: Function to compute noise distribution\n",
        "        map_location: Device mapping for loading the checkpoint\n",
        "        **model_kwargs: Additional arguments for the model constructor\n",
        "\n",
        "    Returns:\n",
        "        Loaded model and training history\n",
        "    \"\"\"\n",
        "    # TODO: Implement the model loading function with these steps:\n",
        "    # 1. Set map_location to the appropriate device\n",
        "    # 2. Check if checkpoint exists\n",
        "    # 3. Load the checkpoint\n",
        "    # 4. Create a model with the same architecture\n",
        "    # 5. Handle parallel models if needed\n",
        "    # 6. Load model weights\n",
        "    # 7. Return the model and training history\n",
        "\n",
        "    # YOUR CODE HERE\n",
        "\n",
        "    if map_location is None:\n",
        "        map_location = device\n",
        "\n",
        "    # Check if checkpoint exists\n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        raise FileNotFoundError(f\"Checkpoint not found at {checkpoint_path}\")\n",
        "\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=map_location)\n",
        "\n",
        "    model = model_class(noise_distribution_fn=noise_distribution_fn, **model_kwargs)\n",
        "    model = model.to(map_location)\n",
        "\n",
        "    if isinstance(model, torch.nn.DataParallel):\n",
        "        model = model.module\n",
        "\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    return model, checkpoint.get('history', [])"
      ],
      "metadata": {
        "id": "r07P1YPXwYKF"
      },
      "execution_count": 230,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Attention Mechanisms for Conditional Generation\n",
        "\n",
        "To enable conditional image generation (generating a specific digit based on a text prompt),\n",
        "we'll enhance our model with attention mechanisms. Attention allows the model to focus on\n",
        "relevant parts of the input when making predictions, and helps bridge the gap between\n",
        "text and image modalities.\n",
        "\n",
        "We'll implement:\n",
        "1. Cross-Attention: To make the image features attend to text features\n",
        "2. Self-Attention: To make each part of the image aware of other parts\n",
        "3. Transformer blocks: Combining attention with feed-forward networks\n",
        "\n",
        "### Task 5.1: Implement the Text Embedding Layer"
      ],
      "metadata": {
        "id": "4qJxab3vwZhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement word embedding for digit labels (0-9)\n",
        "class TextEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    Simple word embedding layer to convert digit labels into vectors.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size=10, embed_dim=256):\n",
        "        super(TextEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, text_indices):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            text_indices: Integer indices representing text tokens [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            Embedded vectors [batch_size, embed_dim]\n",
        "        \"\"\"\n",
        "        return self.embedding(text_indices)"
      ],
      "metadata": {
        "id": "-hLEVa1Mwiyw"
      },
      "execution_count": 231,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.2: Implement the Cross-Attention Mechanism"
      ],
      "metadata": {
        "id": "LCOPiDWZwldW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the cross-attention layer\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Cross-attention layer that allows image features to attend to text features.\n",
        "    \"\"\"\n",
        "    def __init__(self, query_dim, key_dim=None, embed_dim=256, num_heads=1):\n",
        "        \"\"\"\n",
        "        Initialize cross-attention module.\n",
        "\n",
        "        Args:\n",
        "            query_dim: Dimension of query vectors\n",
        "            key_dim: Dimension of key/value vectors (if None, same as query_dim)\n",
        "            embed_dim: Output embedding dimension\n",
        "            num_heads: Number of attention heads (simplified to 1 for clarity)\n",
        "        \"\"\"\n",
        "        super(CrossAttention, self).__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.query_dim = query_dim\n",
        "        self.key_dim = key_dim if key_dim is not None else query_dim\n",
        "\n",
        "        self.to_q = nn.Linear(query_dim, embed_dim)\n",
        "        self.to_k = nn.Linear(self.key_dim, embed_dim)\n",
        "        self.to_v = nn.Linear(self.key_dim, embed_dim)\n",
        "\n",
        "        self.is_self_attention = key_dim is None\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        \"\"\"\n",
        "        Apply attention mechanism.\n",
        "\n",
        "        Args:\n",
        "            x: Query tensor [batch_size, seq_len_q, query_dim]\n",
        "            context: Key/value tensor [batch_size, seq_len_kv, key_dim]\n",
        "                    (if None, uses self-attention)\n",
        "\n",
        "        Returns:\n",
        "            Attended features [batch_size, seq_len_q, query_dim]\n",
        "        \"\"\"\n",
        "        q = self.to_q(x)\n",
        "        k = self.to_k(context if context is not None else x)\n",
        "        v = self.to_v(context if context is not None else x)\n",
        "\n",
        "        attn_scores = torch.bmm(q, k.transpose(-2, -1)) / (self.embed_dim ** 0.5)\n",
        "        attn_weights = torch.softmax(attn_scores, dim=-1)\n",
        "        output = torch.bmm(attn_weights, v)\n",
        "\n",
        "        return output"
      ],
      "metadata": {
        "id": "RXhwkCO9wpim"
      },
      "execution_count": 232,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.3: Implement the Transformer Block"
      ],
      "metadata": {
        "id": "MQpna1OfwvqD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, feature_dim, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.self_attn = nn.MultiheadAttention(feature_dim, num_heads=8)\n",
        "        self.cross_attn = nn.MultiheadAttention(feature_dim, num_heads=8) if context_dim else None\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(feature_dim)\n",
        "        self.norm2 = nn.LayerNorm(feature_dim) if context_dim else None\n",
        "        self.norm3 = nn.LayerNorm(feature_dim)\n",
        "\n",
        "        self.ff_network = nn.Sequential(\n",
        "            nn.Linear(feature_dim, 4 * feature_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * feature_dim, feature_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # Reshape for attention: [batch, seq, dim] -> [seq, batch, dim]\n",
        "        x = x.permute(1, 0, 2)\n",
        "\n",
        "        # Self attention\n",
        "        attn_out, _ = self.self_attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "\n",
        "        # Cross attention if context provided\n",
        "        if context is not None and self.cross_attn is not None:\n",
        "            context = context.permute(1, 0, 2)\n",
        "            attn_out, _ = self.cross_attn(x, context, context)\n",
        "            x = self.norm2(x + attn_out)\n",
        "\n",
        "        # Feed forward\n",
        "        ff_out = self.ff_network(x)\n",
        "        x = self.norm3(x + ff_out)\n",
        "\n",
        "        # Reshape back: [seq, batch, dim] -> [batch, seq, dim]\n",
        "        return x.permute(1, 0, 2)\n"
      ],
      "metadata": {
        "id": "7nhd76bQxKOx"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.4: Implement the Spatial Transformer for 2D Feature Maps"
      ],
      "metadata": {
        "id": "sopclwgnxMP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement a transformer for spatial (image) data\n",
        "class SpatialTransformer(nn.Module):\n",
        "    def __init__(self, feature_dim, context_dim=None):\n",
        "        super().__init__()\n",
        "        self.transformer = TransformerBlock(feature_dim, context_dim)\n",
        "\n",
        "    def forward(self, x, context=None):\n",
        "        # Handle both 3D and 4D inputs\n",
        "        if len(x.shape) == 3:  # [batch, seq_len, features]\n",
        "            return self.transformer(x, context)\n",
        "\n",
        "        # Standard 4D input [batch, channels, height, width]\n",
        "        batch, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape to sequence\n",
        "        x = x.view(batch, channels, height * width).permute(0, 2, 1)  # [batch, h*w, channels]\n",
        "\n",
        "        # Apply transformer\n",
        "        x = self.transformer(x, context)\n",
        "\n",
        "        # Reshape back to spatial\n",
        "        x = x.permute(0, 2, 1).view(batch, channels, height, width)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "CrzUJX0nxQii"
      },
      "execution_count": 258,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 5.5: Implement the Conditional Diffusion U-Net Model"
      ],
      "metadata": {
        "id": "zw7TuKuDxTSI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ConditionalDiffusionUNet(nn.Module):\n",
        "    def __init__(self, noise_distribution_fn, in_channels=1, base_channels=32, num_classes=10):\n",
        "        super().__init__()\n",
        "        self.noise_distribution_fn = noise_distribution_fn\n",
        "\n",
        "        # Time embedding\n",
        "        self.time_embed = nn.Sequential(\n",
        "            nn.Linear(1, 128),\n",
        "            nn.SiLU(),\n",
        "            nn.Linear(128, 256)\n",
        "        )\n",
        "\n",
        "        # Class embedding\n",
        "        self.class_embed = nn.Embedding(num_classes, 256)\n",
        "\n",
        "        # Encoder\n",
        "        self.enc1 = self._block(in_channels, base_channels, 256)\n",
        "        self.enc2 = self._block(base_channels, base_channels*2, 256)\n",
        "        self.enc3 = self._block(base_channels*2, base_channels*4, 256)\n",
        "\n",
        "        # Bottleneck\n",
        "        self.bottleneck = self._block(base_channels*4, base_channels*4, 256)\n",
        "\n",
        "        # Decoder\n",
        "        self.dec1 = self._upblock(base_channels*8, base_channels*2, 256)\n",
        "        self.dec2 = self._upblock(base_channels*4, base_channels, 256)\n",
        "        self.dec3 = self._upblock(base_channels*2, base_channels, 256)\n",
        "\n",
        "        # Final output\n",
        "        self.final = nn.Conv2d(base_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def _block(self, in_ch, out_ch, embed_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.Conv2d(in_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, stride=2, padding=1),\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 1),  # Project embedding\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "    def _upblock(self, in_ch, out_ch, embed_dim):\n",
        "        return nn.Sequential(\n",
        "            nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, padding=1),\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            nn.SiLU(),\n",
        "            nn.Conv2d(out_ch, out_ch, 1),  # Project embedding\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            nn.SiLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        # Time embedding\n",
        "        t_emb = self.time_embed(t.unsqueeze(-1).float())\n",
        "\n",
        "        # Class embedding\n",
        "        if class_labels is not None:\n",
        "            c_emb = self.class_embed(class_labels)\n",
        "            emb = t_emb + c_emb\n",
        "        else:\n",
        "            emb = t_emb\n",
        "\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x + emb.view(emb.shape[0], -1, 1, 1))\n",
        "        e2 = self.enc2(e1 + emb.view(emb.shape[0], -1, 1, 1))\n",
        "        e3 = self.enc3(e2 + emb.view(emb.shape[0], -1, 1, 1))\n",
        "\n",
        "        # Bottleneck\n",
        "        b = self.bottleneck(e3 + emb.view(emb.shape[0], -1, 1, 1))\n",
        "\n",
        "        # Decoder with skip connections\n",
        "        d1 = self.dec1(torch.cat([b, e3], dim=1) + emb.view(emb.shape[0], -1, 1, 1))\n",
        "        d2 = self.dec2(torch.cat([d1, e2], dim=1) + emb.view(emb.shape[0], -1, 1, 1))\n",
        "        d3 = self.dec3(torch.cat([d2, e1], dim=1) + emb.view(emb.shape[0], -1, 1, 1))\n",
        "\n",
        "        # Normalize output\n",
        "        sigma_t = self.noise_distribution_fn(t)[:, None, None, None]\n",
        "        return self.final(d3) / sigma_t"
      ],
      "metadata": {
        "id": "udxS2YY2xgMb"
      },
      "execution_count": 278,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Latent Diffusion with Autoencoders\n",
        "\n",
        "Instead of performing diffusion directly in pixel space, we can operate in a compressed latent space.\n",
        "This approach, introduced in the Stable Diffusion paper, has several advantages:\n",
        "\n",
        "1. Computational efficiency: Working with smaller latent representations is faster\n",
        "2. Better modeling: The latent space may capture more semantic information\n",
        "3. Better quality: Can lead to higher quality generation\n",
        "\n",
        "We'll implement an autoencoder to compress MNIST images to a latent space, then\n",
        "perform diffusion in this compressed representation.\n",
        "\n",
        "### Task 6.1: Implement the Autoencoder Architecture"
      ],
      "metadata": {
        "id": "-Qx0cf6Mxjht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the autoencoder for latent space compression\n",
        "class Autoencoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Autoencoder for compressing images to a latent representation and reconstructing them.\n",
        "    \"\"\"\n",
        "    def __init__(self, channels=[8, 16, 32]):\n",
        "        \"\"\"\n",
        "        Initialize autoencoder with encoder and decoder components.\n",
        "\n",
        "        Args:\n",
        "            channels: Channel dimensions for feature maps at different levels\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Implement encoder network to compress image to latent representation\n",
        "        # The encoder should contain:\n",
        "        # 1. Multiple convolutional layers with downsampling\n",
        "        # 2. Batch normalization\n",
        "        # 3. Activation functions\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        encoder_layers = []\n",
        "        in_ch = 1\n",
        "        for ch in channels:\n",
        "            encoder_layers.extend([\n",
        "                nn.Conv2d(in_ch, ch, 3, stride=2, padding=1),\n",
        "                nn.BatchNorm2d(ch),\n",
        "                nn.ReLU()\n",
        "            ])\n",
        "            in_ch = ch\n",
        "        self.encoder = nn.Sequential(*encoder_layers)\n",
        "\n",
        "        # TODO: Implement decoder network to reconstruct image from latent representation\n",
        "        # The decoder should contain:\n",
        "        # 1. Multiple transpose convolutional layers with upsampling\n",
        "        # 2. Batch normalization\n",
        "        # 3. Activation functions\n",
        "        # 4. Final output activation (e.g., sigmoid for 0-1 normalized images)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "        decoder_layers = []\n",
        "        for i in reversed(range(len(channels))):\n",
        "            out_ch = channels[i-1] if i > 0 else 1\n",
        "            decoder_layers.extend([\n",
        "                nn.ConvTranspose2d(channels[i], out_ch, 3, stride=2, padding=1, output_padding=1),\n",
        "                nn.BatchNorm2d(out_ch) if i > 0 else nn.Identity(),\n",
        "                nn.ReLU() if i > 0 else nn.Sigmoid()\n",
        "            ])\n",
        "        self.decoder = nn.Sequential(*decoder_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Full autoencoder forward pass: encode then decode.\n",
        "\n",
        "        Args:\n",
        "            x: Input images [batch_size, 1, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Reconstructed images [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement the autoencoder forward pass:\n",
        "        # 1. Encode input to latent representation\n",
        "        # 2. Decode latent back to image\n",
        "        # 3. Handle any size mismatch (due to transpose convolutions)\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        latent = self.encoder(x)\n",
        "        reconstructed = self.decoder(latent)\n",
        "        if reconstructed.shape != x.shape:\n",
        "            diff_h = x.shape[2] - reconstructed.shape[2]\n",
        "            diff_w = x.shape[3] - reconstructed.shape[3]\n",
        "            reconstructed = F.pad(reconstructed, [diff_w//2, diff_w - diff_w//2, diff_h//2, diff_h - diff_h//2])\n",
        "        return reconstructed\n",
        "\n",
        "    def encode(self, x):\n",
        "        \"\"\"Encode input to latent representation.\"\"\"\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, latent):\n",
        "        \"\"\"Decode latent representation to image.\"\"\"\n",
        "        return self.decoder(latent)"
      ],
      "metadata": {
        "id": "qOvOn-yCxp0p"
      },
      "execution_count": 236,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 6.2: Implement the Latent Diffusion Model"
      ],
      "metadata": {
        "id": "fSDWboklxsXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Implement the latent diffusion model\n",
        "class LatentDiffusionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Diffusion model operating in the latent space of an autoencoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, autoencoder, noise_distribution_fn,\n",
        "                 latent_channels=32, hidden_channels=[64, 128, 256],\n",
        "                 embed_dim=256, text_dim=256, num_classes=10):\n",
        "        \"\"\"\n",
        "        Initialize latent diffusion model.\n",
        "\n",
        "        Args:\n",
        "            autoencoder: Trained autoencoder model\n",
        "            noise_distribution_fn: Function mapping time to noise std\n",
        "            latent_channels: Number of channels in autoencoder latent space\n",
        "            hidden_channels: Channel dimensions for U-Net layers\n",
        "            embed_dim: Dimension of time embeddings\n",
        "            text_dim: Dimension of text/class embeddings\n",
        "            num_classes: Number of possible conditioning classes\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # TODO: Initialize the latent diffusion model:\n",
        "        # 1. Store the autoencoder (frozen)\n",
        "        # 2. Create a conditional diffusion U-Net for the latent space\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        self.autoencoder = autoencoder\n",
        "        for param in self.autoencoder.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        channels = [latent_channels] + hidden_channels\n",
        "        self.diffusion_model = ConditionalDiffusionUNet(\n",
        "            noise_distribution_fn=noise_distribution_fn,\n",
        "            channels=channels,\n",
        "            in_channels=32,\n",
        "            embed_dim=embed_dim,\n",
        "            text_dim=text_dim,\n",
        "            num_classes=num_classes\n",
        "        )\n",
        "\n",
        "    def forward(self, x, t, class_labels=None):\n",
        "        \"\"\"\n",
        "        Forward pass for training.\n",
        "\n",
        "        Args:\n",
        "            x: Input latent representations [batch_size, latent_channels, h, w]\n",
        "            t: Diffusion timesteps [batch_size]\n",
        "            class_labels: Optional conditioning labels [batch_size]\n",
        "\n",
        "        Returns:\n",
        "            Predicted score/noise\n",
        "        \"\"\"\n",
        "        return self.diffusion_model(x, t, class_labels)\n",
        "\n",
        "    def encode_images(self, images):\n",
        "        \"\"\"\n",
        "        Encode images to latent representations.\n",
        "\n",
        "        Args:\n",
        "            images: Input images [batch_size, 1, height, width]\n",
        "\n",
        "        Returns:\n",
        "            Latent representations [batch_size, latent_channels, h', w']\n",
        "        \"\"\"\n",
        "        return self.autoencoder.encode(images)\n",
        "\n",
        "    def decode_latents(self, latents):\n",
        "        \"\"\"\n",
        "        Decode latent representations to images.\n",
        "\n",
        "        Args:\n",
        "            latents: Latent representations [batch_size, latent_channels, h', w']\n",
        "\n",
        "        Returns:\n",
        "            Reconstructed images [batch_size, 1, height, width]\n",
        "        \"\"\"\n",
        "        return self.autoencoder.decode(latents)\n",
        "\n",
        "    def generate(self, num_samples, class_labels=None, num_steps=500, latent_shape=None):\n",
        "        \"\"\"\n",
        "        Generate images from noise via latent diffusion.\n",
        "\n",
        "        Args:\n",
        "            num_samples: Number of images to generate\n",
        "            class_labels: Optional conditioning labels [num_samples]\n",
        "            num_steps: Number of diffusion sampling steps\n",
        "            latent_shape: Shape of latent representations\n",
        "\n",
        "        Returns:\n",
        "            Generated images [num_samples, 1, height, width]\n",
        "        \"\"\"\n",
        "        # TODO: Implement image generation with these steps:\n",
        "        # 1. Determine latent shape (if not provided)\n",
        "        # 2. Generate latents using the diffusion model\n",
        "        # 3. Decode latents to images\n",
        "\n",
        "        # YOUR CODE HERE\n",
        "\n",
        "        if latent_shape is None:\n",
        "            latent_shape = (num_samples, 32, 8, 8)\n",
        "\n",
        "        device = next(self.parameters()).device\n",
        "        latents = torch.randn(latent_shape, device=device)\n",
        "\n",
        "        for step in reversed(range(num_steps)):\n",
        "            t = torch.full((num_samples,), step/num_steps, device=device)\n",
        "            noise_pred = self.diffusion_model(latents, t, class_labels)\n",
        "            alpha = 1 / num_steps\n",
        "            latents = latents - alpha * noise_pred\n",
        "\n",
        "        return self.decode_latents(latents)"
      ],
      "metadata": {
        "id": "t6LJ43RAyJot"
      },
      "execution_count": 237,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Putting It All Together: Training and Evaluation\n",
        "\n",
        "In this section, you'll implement functions to train and evaluate your diffusion models.\n"
      ],
      "metadata": {
        "id": "vSQ_EiLuySzJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These functions are provided. You can modify them for your needs or create new ones.\n",
        "\n",
        "def train_and_evaluate_basic_model(train_model=False):\n",
        "    \"\"\"Train and evaluate a basic unconditioned diffusion model.\"\"\"\n",
        "    print(\"\\n1. Basic diffusion model (unconditioned)...\\n\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    mnist_train = MNIST('.', train=True, transform=transform, download=True)\n",
        "    mnist_test = MNIST('.', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Model path\n",
        "    model_path = os.path.join(CHECKPOINT_DIR, \"basic_diffusion.pt\")\n",
        "\n",
        "    if train_model:\n",
        "        # Create and train the model\n",
        "        basic_model = torch.nn.DataParallel(DiffusionUNet(noise_distribution_fn=noise_distribution_fn))\n",
        "        basic_model = basic_model.to(device)\n",
        "\n",
        "        print(f\"Training basic diffusion model...\")\n",
        "        basic_model, history = train_diffusion_model(\n",
        "            basic_model,\n",
        "            mnist_train,\n",
        "            noise_distribution_fn,\n",
        "            num_epochs=30,\n",
        "            batch_size=256,\n",
        "            learning_rate=1e-4,\n",
        "            checkpoint_path=model_path,\n",
        "            resume_training=False\n",
        "        )\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([h['epoch'] for h in history], [h['loss'] for h in history])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss')\n",
        "        plt.savefig(os.path.join(CHECKPOINT_DIR, \"basic_model_loss.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    try:\n",
        "        # Load model from checkpoint\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        basic_model, _ = load_model_from_checkpoint(\n",
        "            DiffusionUNet,\n",
        "            model_path,\n",
        "            noise_distribution_fn\n",
        "        )\n",
        "\n",
        "        # Generate samples\n",
        "        print(\"Generating samples...\")\n",
        "        samples = euler_maruyama_sampler(\n",
        "            basic_model,\n",
        "            noise_distribution_fn,\n",
        "            diffusion_coeff_fn,\n",
        "            batch_size=25,\n",
        "            num_steps=200,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "        # Save samples\n",
        "        save_path = os.path.join(CHECKPOINT_DIR, \"basic_model_samples.png\")\n",
        "        save_samples(samples, save_path)\n",
        "        print(f\"Samples saved to {save_path}\")\n",
        "\n",
        "        return basic_model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Model checkpoint not found at {model_path}. Train the model first with train_model=True.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_and_evaluate_conditional_model(train_model=False):\n",
        "    \"\"\"Train and evaluate a conditional diffusion model with attention.\"\"\"\n",
        "    print(\"\\n2. Conditional diffusion model with attention...\\n\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    mnist_train = MNIST('.', train=True, transform=transform, download=True)\n",
        "    mnist_test = MNIST('.', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Model path\n",
        "    model_path = os.path.join(CHECKPOINT_DIR, \"conditional_diffusion.pt\")\n",
        "\n",
        "    if train_model:\n",
        "        # Create and train the model\n",
        "        cond_model = torch.nn.DataParallel(ConditionalDiffusionUNet(\n",
        "            noise_distribution_fn=noise_distribution_fn,\n",
        "            channels=[32, 64, 128, 256],\n",
        "            in_channels=32,\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        ))\n",
        "        cond_model = cond_model.to(device)\n",
        "\n",
        "        print(f\"Training conditional diffusion model...\")\n",
        "        cond_model, history = train_diffusion_model(\n",
        "            cond_model,\n",
        "            mnist_train,\n",
        "            noise_distribution_fn,\n",
        "            num_epochs=50,\n",
        "            batch_size=256,\n",
        "            learning_rate=1e-4,\n",
        "            conditional=True,\n",
        "            checkpoint_path=model_path,\n",
        "            resume_training=False\n",
        "        )\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([h['epoch'] for h in history], [h['loss'] for h in history])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss (Conditional Model)')\n",
        "        plt.savefig(os.path.join(CHECKPOINT_DIR, \"conditional_model_loss.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    try:\n",
        "        # Load model from checkpoint\n",
        "        print(f\"Loading model from {model_path}\")\n",
        "        cond_model, _ = load_model_from_checkpoint(\n",
        "            ConditionalDiffusionUNet,\n",
        "            model_path,\n",
        "            noise_distribution_fn,\n",
        "            channels=[32, 64, 128, 256],\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        )\n",
        "\n",
        "        # Generate samples for each digit\n",
        "        for digit in range(10):\n",
        "            print(f\"Generating samples for digit {digit}...\")\n",
        "\n",
        "            # Create labels tensor for the digit\n",
        "            labels = torch.ones(25, dtype=torch.long, device=device) * digit\n",
        "\n",
        "            # Generate samples\n",
        "            samples = euler_maruyama_sampler(\n",
        "                cond_model,\n",
        "                noise_distribution_fn,\n",
        "                diffusion_coeff_fn,\n",
        "                batch_size=25,\n",
        "                num_steps=200,\n",
        "                device=device,\n",
        "                class_labels=labels\n",
        "            )\n",
        "\n",
        "            # Save samples\n",
        "            save_path = os.path.join(CHECKPOINT_DIR, f\"conditional_model_samples_digit{digit}.png\")\n",
        "            save_samples(samples, save_path)\n",
        "            print(f\"Samples for digit {digit} saved to {save_path}\")\n",
        "\n",
        "        return cond_model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Model checkpoint not found at {model_path}. Train the model first with train_model=True.\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def train_and_evaluate_latent_model(train_model=False, train_autoencoder=False):\n",
        "    \"\"\"Train and evaluate a latent diffusion model.\"\"\"\n",
        "    print(\"\\n3. Latent diffusion model with autoencoder...\\n\")\n",
        "\n",
        "    # Load MNIST dataset\n",
        "    transform = transforms.ToTensor()\n",
        "    mnist_train = MNIST('.', train=True, transform=transform, download=True)\n",
        "    mnist_test = MNIST('.', train=False, transform=transform, download=True)\n",
        "\n",
        "    # Paths for saving models\n",
        "    ae_path = os.path.join(CHECKPOINT_DIR, \"autoencoder.pt\")\n",
        "    latent_model_path = os.path.join(CHECKPOINT_DIR, \"latent_diffusion.pt\")\n",
        "\n",
        "    # First train or load the autoencoder\n",
        "    autoencoder = Autoencoder(channels=[8, 16, 32]).to(device)\n",
        "\n",
        "    if train_autoencoder:\n",
        "        # Define loss function (MSE) and optimizer\n",
        "        print(\"Training autoencoder...\")\n",
        "        mse_loss = nn.MSELoss()\n",
        "        optimizer = Adam(autoencoder.parameters(), lr=1e-3)\n",
        "\n",
        "        # Setup data loader\n",
        "        data_loader = DataLoader(mnist_train, batch_size=256, shuffle=True, num_workers=4)\n",
        "\n",
        "        # Training loop\n",
        "        num_epochs = 20\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_loss = 0.0\n",
        "            num_samples = 0\n",
        "\n",
        "            # Process batches\n",
        "            batch_progress = tqdm(data_loader, desc=f\"AE Epoch {epoch+1}/{num_epochs}\", leave=False)\n",
        "            for x, _ in batch_progress:\n",
        "                x = x.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                x_recon = autoencoder(x)\n",
        "\n",
        "                # Calculate loss\n",
        "                loss = mse_loss(x_recon, x)\n",
        "\n",
        "                # Backward and optimize\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # Track statistics\n",
        "                epoch_loss += loss.item() * x.shape[0]\n",
        "                num_samples += x.shape[0]\n",
        "\n",
        "                # Update batch progress\n",
        "                batch_progress.set_postfix({\"Batch Loss\": f\"{loss.item():.6f}\"})\n",
        "\n",
        "            # Print epoch results\n",
        "            avg_loss = epoch_loss / num_samples\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.6f}\")\n",
        "\n",
        "        # Save trained autoencoder\n",
        "        torch.save(autoencoder.state_dict(), ae_path)\n",
        "        print(f\"Autoencoder saved to {ae_path}\")\n",
        "\n",
        "        # Visualize reconstructions\n",
        "        with torch.no_grad():\n",
        "            test_samples = next(iter(DataLoader(mnist_test, batch_size=16)))[0].to(device)\n",
        "            reconstructions = autoencoder(test_samples)\n",
        "\n",
        "            # Create comparison grid\n",
        "            comparison = torch.cat([test_samples, reconstructions])\n",
        "            save_path = os.path.join(CHECKPOINT_DIR, \"autoencoder_reconstructions.png\")\n",
        "            save_samples(comparison, save_path)\n",
        "            print(f\"Reconstruction examples saved to {save_path}\")\n",
        "    else:\n",
        "        # Load pre-trained autoencoder\n",
        "        try:\n",
        "            autoencoder.load_state_dict(torch.load(ae_path, map_location=device))\n",
        "            print(f\"Loaded autoencoder from {ae_path}\")\n",
        "        except FileNotFoundError:\n",
        "            print(f\"Autoencoder checkpoint not found at {ae_path}. Train it first with train_autoencoder=True.\")\n",
        "            return None\n",
        "\n",
        "    # Create latent dataset\n",
        "    if train_model:\n",
        "        print(\"Creating latent dataset...\")\n",
        "        data_loader = DataLoader(mnist_train, batch_size=256, shuffle=False, num_workers=4)\n",
        "\n",
        "        all_latents = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in tqdm(data_loader, desc=\"Encoding dataset to latents\"):\n",
        "                x = x.to(device)\n",
        "                latents = autoencoder.encode(x)\n",
        "                all_latents.append(latents.cpu())\n",
        "                all_labels.append(y)\n",
        "\n",
        "        latent_data = torch.cat(all_latents, dim=0)\n",
        "        label_data = torch.cat(all_labels, dim=0)\n",
        "\n",
        "        # Create dataset\n",
        "        latent_dataset = TensorDataset(latent_data, label_data)\n",
        "\n",
        "        # Get latent dimensions for model creation\n",
        "        sample_latent_shape = all_latents[0].shape[1:]\n",
        "        latent_channels = sample_latent_shape[0]\n",
        "\n",
        "        # Create and train latent diffusion model\n",
        "        latent_model = LatentDiffusionModel(\n",
        "            autoencoder=autoencoder,\n",
        "            noise_distribution_fn=noise_distribution_fn,\n",
        "            latent_channels=latent_channels,\n",
        "            hidden_channels=[64, 128, 256],\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        ).to(device)\n",
        "\n",
        "        print(f\"Training latent diffusion model...\")\n",
        "        # We train just the diffusion part, not the autoencoder\n",
        "        # We're using the same training function but passing the diffusion model directly\n",
        "        latent_diffusion, history = train_diffusion_model(\n",
        "            latent_model.diffusion_model,\n",
        "            latent_dataset,\n",
        "            noise_distribution_fn,\n",
        "            # num_epochs=50,\n",
        "            num_epochs=1,\n",
        "            batch_size=256,\n",
        "            learning_rate=1e-4,\n",
        "            conditional=True,\n",
        "            checkpoint_path=latent_model_path,\n",
        "            resume_training=False\n",
        "        )\n",
        "\n",
        "        # Update the diffusion model part of latent_model\n",
        "        latent_model.diffusion_model = latent_diffusion\n",
        "\n",
        "        # Plot training loss\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot([h['epoch'] for h in history], [h['loss'] for h in history])\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Training Loss (Latent Diffusion)')\n",
        "        plt.savefig(os.path.join(CHECKPOINT_DIR, \"latent_model_loss.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    try:\n",
        "        # Load model from checkpoint\n",
        "        print(f\"Loading diffusion model from {latent_model_path}\")\n",
        "        # First get the latent dimensions\n",
        "        with torch.no_grad():\n",
        "            sample_x = torch.zeros(1, 1, 28, 28, device=device)\n",
        "            sample_latent = autoencoder.encode(sample_x)\n",
        "            latent_channels = sample_latent.shape[1]\n",
        "            latent_shape = (latent_channels, sample_latent.shape[2], sample_latent.shape[3])\n",
        "\n",
        "        # Create a latent diffusion model\n",
        "        latent_model = LatentDiffusionModel(\n",
        "            autoencoder=autoencoder,\n",
        "            noise_distribution_fn=noise_distribution_fn,\n",
        "            latent_channels=latent_channels,\n",
        "            hidden_channels=[64, 128, 256],\n",
        "            embed_dim=256,\n",
        "            text_dim=128,\n",
        "            num_classes=10\n",
        "        ).to(device)\n",
        "\n",
        "        # Load just the diffusion part\n",
        "        checkpoint = torch.load(latent_model_path, map_location=device)\n",
        "        latent_model.diffusion_model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "        # Generate samples for each digit\n",
        "        for digit in range(10):\n",
        "            print(f\"Generating samples for digit {digit} using latent diffusion...\")\n",
        "\n",
        "            # Generate using the helper method that handles the entire pipeline\n",
        "            samples = latent_model.generate(\n",
        "                num_samples=25,\n",
        "                class_labels=torch.ones(25, dtype=torch.long, device=device) * digit,\n",
        "                num_steps=200,\n",
        "                latent_shape=latent_shape\n",
        "            )\n",
        "\n",
        "            # Save samples\n",
        "            save_path = os.path.join(CHECKPOINT_DIR, f\"latent_model_samples_digit{digit}.png\")\n",
        "            save_samples(samples, save_path)\n",
        "            print(f\"Samples for digit {digit} saved to {save_path}\")\n",
        "\n",
        "        return latent_model\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Latent diffusion model checkpoint not found at {latent_model_path}. Train it first with train_model=True.\")\n",
        "        return None"
      ],
      "metadata": {
        "id": "6YlabbX-ybaW"
      },
      "execution_count": 267,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task 7: Run and Experiment\n",
        "\n",
        "After implementing all the required components, you can run your models and experiment with different\n",
        "parameters to observe their effects on the generated images."
      ],
      "metadata": {
        "id": "BfqIh_7XydzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Choose which models to train and evaluate\n",
        "    # Set parameters to True to train the models, or False to load pre-trained models\n",
        "\n",
        "    # TODO: Uncomment the models you want to train/evaluate\n",
        "\n",
        "    # 1. Basic diffusion model (no conditioning)\n",
        "    # train_and_evaluate_basic_model(train_model=True)\n",
        "\n",
        "    # 2. Conditional diffusion model with attention\n",
        "    # train_and_evaluate_conditional_model(train_model=True)\n",
        "\n",
        "    # 3. Latent diffusion model\n",
        "    # train_and_evaluate_latent_model(train_autoencoder=True, train_model=True)\n",
        "    train_and_evaluate_latent_model(train_autoencoder=False, train_model=True)\n",
        "\n",
        "    # Or just evaluate pre-trained models:\n",
        "    # train_and_evaluate_basic_model(train_model=False)\n",
        "    # train_and_evaluate_conditional_model(train_model=False)\n",
        "    # train_and_evaluate_latent_model(train_model=False, train_autoencoder=False)\n",
        "    # train_and_evaluate_latent_model(train_model=False, train_autoencoder=True)\n",
        "\n",
        "    print(\"\\nDiffusion Model Lab completed! Check the 'model_checkpoints' directory for results.\")\n",
        "\n",
        "    \"\"\"\n",
        "    ## Lab Extensions (Optional)\n",
        "\n",
        "    If you've completed all the tasks and want to explore further, you can try these extensions:\n",
        "\n",
        "    1. Compare the quality and training times of the different approaches\n",
        "    2. Experiment with different noise schedules and model architectures\n",
        "    3. Apply the model to a different dataset (e.g., FashionMNIST)\n",
        "    4. Implement classifier-free guidance for improved conditional generation\n",
        "    5. Add more control to the generation process (e.g., controlling digit size or style)\n",
        "    \"\"\"\n"
      ],
      "metadata": {
        "id": "dQ_gtlsix47A",
        "outputId": "46151696-9ed7-4a27-dba5-6953956db901",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 529
        }
      },
      "execution_count": 279,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "3. Latent diffusion model with autoencoder...\n",
            "\n",
            "Loaded autoencoder from model_checkpoints/autoencoder.pt\n",
            "Creating latent dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Encoding dataset to latents: 100%|██████████| 235/235 [00:05<00:00, 40.06it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ConditionalDiffusionUNet.__init__() got an unexpected keyword argument 'channels'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-279-3d00144db417>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# 3. Latent diffusion model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m# train_and_evaluate_latent_model(train_autoencoder=True, train_model=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtrain_and_evaluate_latent_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_autoencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Or just evaluate pre-trained models:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-267-0b8187488a82>\u001b[0m in \u001b[0;36mtrain_and_evaluate_latent_model\u001b[0;34m(train_model, train_autoencoder)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;31m# Create and train latent diffusion model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 269\u001b[0;31m         latent_model = LatentDiffusionModel(\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mautoencoder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m             \u001b[0mnoise_distribution_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_distribution_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-237-668576672133>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, autoencoder, noise_distribution_fn, latent_channels, hidden_channels, embed_dim, text_dim, num_classes)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mchannels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlatent_channels\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mhidden_channels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         self.diffusion_model = ConditionalDiffusionUNet(\n\u001b[0m\u001b[1;32m     35\u001b[0m             \u001b[0mnoise_distribution_fn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoise_distribution_fn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mchannels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchannels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: ConditionalDiffusionUNet.__init__() got an unexpected keyword argument 'channels'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "qRhdoZ83JvPl"
      }
    }
  ]
}